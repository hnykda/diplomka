Automatically generated by Mendeley Desktop 1.17.11
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Wikipedia,
author = {Wikipedia, From},
file = {:home/dan/edu/mendeley/kapitola2.pdf:pdf},
pages = {2--6},
title = {{Sparse dictionary learning}}
}
@article{applied_dec,
author = {Raiffa, Howard and Schlaifer, Robert},
doi = {10.1002/bs.3830070108},
issn = {1099-1743},
journal = {Behavioral Science},
number = {1},
pages = {103--104},
publisher = {John Wiley {\&} Sons, Ltd.},
title = {{Applied statistical decision theory}},
url = {http://dx.doi.org/10.1002/bs.3830070108},
volume = {7},
year = {1962}
}
@book{Upton2008,
abstract = {This wide-ranging, jargon-free dictionary contains over 2,000 entries on all aspects of statistics including terms used in computing, mathematics, operational research, and probability, as well as biographical information on over 200 key figures in the field, and coverage of statistical journals and societies. It embraces the whole multi-disciplinary spectrum of this complex subject, and will be invaluable for students and professionals from a wide range of disciplines, including politics, market research, medicine, psychology, pharmaceuticals, and mathematics. The entries are generously illustrated with useful figures and diagrams, and include worked examples where applicable, which place them in a practical context. Fully updated for the second edition, the dictionary now boasts over 200 new entries including over 30 new biographies, as well as internet links which point to useful sites for further information, and many additional illustrative examples that clarify terms by showing them in use.},
author = {Upton, G. and Cook, I.},
booktitle = {Oxford university press},
doi = {10.1093/acref/9780199541454.001.0001},
isbn = {978-0-19-954145-4},
pages = {512},
pmid = {15493422},
title = {{A Dictionary of Statistics}},
year = {2008}
}
@book{Parisi.1988,
author = {Parisi, Giorgio},
title = {{Statistical Field Theory}},
year = {1988}
}
@article{LessWrong2008,
annote = {Chyb{\'{i}} jak he Simple Truth tak An Intuitive Explanation of Bayes' Theorem},
author = {LessWrong},
file = {:home/dan/edu/mendeley/LessWrong - 2008 - Map and Territory.pdf:pdf},
title = {{Map and Territory}},
year = {2008}
}
@article{MacQueen1967,
abstract = {Some methods for classification and analysis of multivariate observations},
author = {MacQueen, James},
file = {:home/dan/edu/mendeley/J. MacQueen - 1967 - Some methods for classification and analysis of multivariate observations.pdf:pdf},
journal = {Proceedings of the Berkeley Symposium on Mathematical Statistics and Probability},
pages = {281--297},
publisher = {University of California Press},
title = {{Some methods for classification and analysis of multivariate observations}},
url = {https://projecteuclid.org/euclid.bsmsp/1200512992},
volume = {1},
year = {1967}
}
@article{E.T.Jaynes1986,
abstract = {We note the main points of history, as a framework on which to hang many background remarks concerning the nature and motivation of Bayesian/Maximum Entropy methods. Experience has shown that these are needed in order to understand recentwork and problems. A more complete account of the history, with many more details and references, is given in Jaynes (1978). The following discussion is essentially nontechnical; the aim is only to convey a little introductory $\backslash$feel" for our outlook, purpose, and terminology, and to alert newcomers to common pitfalls of misunderstanding.},
author = {{E.T. Jaynes}},
doi = {10.1017/CBO9780511569678},
file = {:home/dan/edu/mendeley/general.background (1).pdf:pdf},
isbn = {9780511569678},
journal = {Maximum Entropy and Bayesian Methods in Applied Statistics},
keywords = {Ars Conjectandi,Barnard,Bayes' theorem,Clausius entropy,Cox,Cramer,Darwin,Desideratum of Consistency,Fowler,Gibbs,Gull,Herodotus,Jaynes,Johnson,Jupiter,Kolmogorov,Lindley,Liouville,Mathematical Methods,Poincare,Rissanen,Saturn,Shannon entropy,Shore,Skilling,Sommerfeld,Statistics for physical sciences and engineering,Tower of Babel,Tukey,Twenty Questions,Wald,crystallography,difference,digital,entropy,filters,finite,likelihood,standard deviation,time series},
number = {August 1984},
pages = {1--25},
title = {{Bayesian Methods: General Background}},
url = {/chapter.jsf?bid=CBO9780511569678A007{\&}cid=CBO9780511569678A007},
year = {1986}
}
@book{finettitheory,
author = {de Finetti, B},
publisher = {J. Wiley {\&} Sons, Inc., New York},
title = {{Theory of Probability: A critical introductory treatment}},
year = {1975}
}
@inproceedings{MairalONDL,
abstract = {Sparse coding—that is, modelling data vectors as sparse linear combinations of basis elements—is widely used in machine learning, neuroscience, signal processing, and statistics. This paper fo- cuses on learning the basis set, also called dic- tionary, to adapt it to specific data, an approach that has recently proven to be very effective for signal reconstruction and classification in the au- dio and image processing domains. This paper proposes a new online optimization algorithm for dictionary learning, based on stochastic ap- proximations, which scales up gracefully to large datasets with millions of training samples. A proof of convergence is presented, along with experiments with natural images demonstrating that it leads to faster performance and better dic- tionaries than classical batch algorithms for both small and large datasets.},
address = {New York, NY, USA},
archivePrefix = {arXiv},
arxivId = {0908.0050},
author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning - ICML '09},
doi = {10.1145/1553374.1553463},
eprint = {0908.0050},
file = {:home/dan/edu/mendeley/Bishop - Pattern Recognition And Machine Learning - Springer 2006.pdf:pdf},
isbn = {9781605585161},
issn = {0016450X},
pages = {1--8},
pmid = {710806},
publisher = {ACM},
series = {ICML '09},
title = {{Online dictionary learning for sparse coding}},
url = {http://portal.acm.org/citation.cfm?doid=1553374.1553463},
year = {2009}
}
@article{Winn2005,
abstract = {This paper presents Variational Message Passing (VMP), a general purpose algorithm for applying variational inference to a Bayesian Network. Like belief propagation, Variational Message Passing proceeds by passing messages between nodes in the graph and updating posterior beliefs using local operations at each node. Each such update increases a lower bound on the log evidence (unless already at a local maximum). In contrast to belief propagation, VMP can be applied to a very general class of conjugate-exponential models because it uses a factorised variational approximation. Furthermore, by introducing ad- ditional variational parameters, VMP can be applied to models containing non-conjugate distributions. The VMP framework also allows the lower bound to be evaluated, and this can be used both for model comparison and for detection of convergence. Variational Mes- sage Passing has been implemented in the form of a general purpose inference engine called VIBES (‘Variational Inference for BayEsian networkS') which allows models to be specified graphically and then solved variationally without recourse to coding.},
author = {Winn, John and Bishop, Cm and Jaakkola, T},
doi = {10.1007/s002130100880},
file = {:home/dan/edu/mendeley/Winn, Bishop - 2005 - Variational Message Passing.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {bayesian networks,variational methods},
pages = {661--694},
pmid = {11713616},
title = {{Variational Message Passing}},
volume = {6},
year = {2005}
}
@article{Edwards,
author = {Edwards, Paul N},
file = {:home/dan/edu/mendeley/Edwards - Unknown - How to Read a Book, v5.0.pdf:pdf},
pages = {0--10},
title = {{How to Read a Book, v5.0}}
}
@article{Zhou2009,
author = {Zhou, Mingyuan and Chen, Haojun and Paisley, John and Ren, Lu and Sapiro, Guillermo and Carin, Lawrence},
file = {:home/dan/edu/mendeley/Model and Inference.pdf:pdf},
journal = {Nips},
number = {2},
pages = {1--4},
title = {{Non-Parametric Bayesian Dictionary Learning for Sparse Image Representations: Model and Inference}},
year = {2009}
}
@inproceedings{Stuart1994,
author = {Stuart, A and Ord, K},
title = {{Kendall's Advanced Theory of Statistics: Volume I—Distribution Theory}},
year = {1994}
}
@article{sparsesolsl1,
author = {Donoho, David L},
doi = {10.1002/cpa.20132},
issn = {1097-0312},
journal = {Communications on Pure and Applied Mathematics},
number = {6},
pages = {797--829},
publisher = {Wiley Subscription Services, Inc., A Wiley Company},
title = {{For most large underdetermined systems of linear equations the minimal l1-norm solution is also the sparsest solution}},
url = {http://dx.doi.org/10.1002/cpa.20132},
volume = {59},
year = {2006}
}
@incollection{nonparabayes,
author = {Zhou, Mingyuan and Chen, Haojun and Paisley, John William and Ren, Lu and Sapiro, Guillermo and Carin, Lawrence and Paisley, John William},
booktitle = {Advances in Neural Information Processing Systems 22},
editor = {Bengio, Y and Schuurmans, D and Lafferty, J and Williams, C and Culotta, A},
file = {:home/dan/edu/mendeley/Mingyuan{\_}nips2009{\_}final.pdf:pdf},
pages = {2295--2303},
title = {{Non-Parametric Bayesian Dictionary Learning for Sparse Image Representations}},
url = {http://books.nips.cc/papers/files/nips22/NIPS2009{\_}0190.pdf},
year = {2009}
}
@article{Blei2017,
abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
archivePrefix = {arXiv},
arxivId = {arXiv:1601.00670v6},
author = {Blei, David M and Kucukelbir, Alp and Mcauliffe, Jon D},
eprint = {arXiv:1601.00670v6},
file = {:home/dan/edu/mendeley/Blei, Kucukelbir, Mcauliffe - 2017 - Variational Inference A Review for Statisticians.pdf:pdf},
keywords = {Algorithms,Computationally Intensive Methods,Statistical Computing},
title = {{Variational Inference: A Review for Statisticians}},
url = {https://arxiv.org/pdf/1601.00670.pdf},
year = {2017}
}
@article{Dupre2004,
abstract = {By basing Bayesian probability theory on five axioms, we can give a trivial proof of Cox's Theorem on the product rule and sum rule for conditional plausibility without assuming continuity or differentiablity of plausibility. Instead, we extend the notion of plausibility to apply to unknowns, giving them plausi-ble values. Thus, we combine the best aspects of two approaches to Bayesian probability theory, namely the Cox-Jaynes theory and the de Finetti theory.},
author = {Dupr{\'{e}}, Maurice J and Tipler, Frank J},
file = {:home/dan/edu/mendeley/Dupr{\'{e}}, Tipler - 2004 - New Axioms for Rigorous Bayesian Probability.pdf:pdf},
journal = {Bayesian Analysis},
keywords = {Axiomatic Bayesian Probability,Cox,Jaynes,Product Rule,Sum Rule,de Finetti},
number = {1},
pages = {1--8},
title = {{New Axioms for Rigorous Bayesian Probability}},
url = {http://dauns.math.tulane.edu/{~}dupre/OLDPUBLIC/20090804 BA FNL 3.pdf},
volume = {1},
year = {2004}
}
@book{LessWrong2013,
author = {LessWrong},
file = {:home/dan/edu/mendeley/LessWrong - 2013 - LessWrong.com Sequences.pdf:pdf},
title = {{LessWrong.com Sequences}},
year = {2013}
}
@article{Jaynes2003,
abstract = {The standard rules of probability can be interpreted as uniquely valid principles in logic. In this book, E. T. Jaynes dispels the imaginary distinction between 'probability theory' and 'statistical inference', leaving a logical unity and simplicity, which provides greater technical power and flexibility in applications. This book goes beyond the conventional mathematics of probability theory, viewing the subject in a wider context. New results are discussed, along with applications of probability theory to a wide variety of problems in physics, mathematics, economics, chemistry and biology. It contains many exercises and problems, and is suitable for use as a textbook on graduate level courses involving data analysis. The material is aimed at readers who are already familiar with applied mathematics at an advanced undergraduate level or higher. The book will be of interest to scientists working in any area where inference from incomplete information is necessary.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Jaynes, E. T.},
doi = {10.1007/BF02985800},
eprint = {arXiv:1011.1669v3},
isbn = {0521592712},
issn = {0343-6993},
journal = {The Mathematical Intelligencer},
number = {2},
pages = {83--83},
pmid = {4362089},
title = {{Probability Theory: The Logic of Science.}},
volume = {27},
year = {2003}
}
@article{Blei2006,
abstract = {Dirichlet process (DP) mixture models are the cornerstone of nonparametric Bayesian statistics, and the development of Monte-Carlo Markov chain (MCMC) sampling methods for DP mixtures has enabled the application of nonparametric Bayesian methods to a variety of practical data analysis problems. However, MCMC sampling can be prohibitively slow, and it is important to explore alternatives. One class of alternatives is provided by variational methods, a class of deterministic algorithms that convert inference problems into optimization problems (Opper and Saad 2001; Wainwright and Jordan 2003). Thus far, variational methods have mainly been explored in the parametric setting, in particular within the formalism of the exponential family (Attias 2000; Ghahramani and Beal 2001; Blei et al. 2003). In this paper, we present a variational inference algorithm for DP mixtures. We present experiments that compare the algorithm to Gibbs sampling algorithms for DP mixtures of Gaussians and present an application to a large-scale image analysis problem.},
author = {Blei, David M. and Jordan, Michael I.},
doi = {10.1214/06-BA104},
file = {:home/dan/edu/mendeley/Blei, Jordan - 2006 - Variational inference for Dirichlet process mixtures.pdf:pdf},
isbn = {1936-0975},
issn = {19360975},
journal = {Bayesian Analysis},
keywords = {Bayesian computation,Dirichlet processes,Hierarchical models,Image processing,Variational inference},
number = {1 A},
pages = {121--144},
title = {{Variational inference for Dirichlet process mixtures}},
volume = {1},
year = {2006}
}
@misc{LessWrong2014,
author = {LessWrong},
file = {:home/dan/edu/mendeley/LessWrong - 2014 - An abridged introduction to Less Wrong.pdf:pdf},
keywords = {Less Wrong},
title = {{An abridged introduction to Less Wrong}},
year = {2014}
}
@article{Tracy2001,
author = {Tracy, B},
file = {:home/dan/edu/mendeley/Tracy - 2001 - Eat That Frog!.pdf:pdf},
isbn = {1583762027},
pages = {1--113},
title = {{Eat That Frog!}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=A10sRE{\_}q-ScC{\&}oi=fnd{\&}pg=PR9{\&}dq=Eat+That+Frog{\&}ots=x0sIiVN2Dk{\&}sig=iJzHlt85cfpi-HHHVMXqdLnmymA},
year = {2001}
}
@article{Griffiths2011,
abstract = {The Indian buffet process is a stochastic process defining a probability distribution over equiva- lence classes of sparse binary matrices with a finite number of rows and an unbounded number of columns. This distribution is suitable for use as a prior in probabilisticmodels that represent objects using a potentially infinite array of features, or that involve bipartite graphs in which the size of at least one class of nodes is unknown. We give a detailed derivation of this distribution, and illustrate its use as a prior in an infinite latent featuremodel. We then review recent applications of the Indian buffet process in machine learning, discuss its extensions, and summarize its connections to other stochastic processes.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Griffiths, Thomas L. and Ghahramani, Zoubin},
doi = {10.1016/j.biotechadv.2011.08.021.Secreted},
eprint = {NIHMS150003},
file = {:home/dan/edu/mendeley/ibf{\_}review.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal ofMachine Learning Research},
keywords = {beta process,chinese,exchangeable distributions,latent variable models,markov chain monte carlo,nonparametric bayes,restaurant processes,sparse binary matrices},
pages = {1185--1224},
pmid = {290096100001},
title = {{The Indian Buffet Process: An Introduction and Review}},
volume = {12},
year = {2011}
}
@article{Gershman2011,
abstract = {A key problem in statistical modeling is model selection, how to choose a model at an appropriate level of complexity. This problem appears in many settings, most prominently in choosing the number of clusters in mixture models or the number of factors in factor analysis. In this tutorial we describe Bayesian nonparametric methods, a class of methods that side-steps this issue by allowing the data to determine the complexity of the model. This tutorial is a high-level introduction to Bayesian nonparametric methods and contains several examples of their application.},
author = {Gershman, Samuel J and Blei, David M},
file = {:home/dan/edu/mendeley/Gershman, Blei - 2011 - A Tutorial on Bayesian Nonparametric Models.pdf:pdf},
title = {{A Tutorial on Bayesian Nonparametric Models}},
url = {https://arxiv.org/pdf/1106.2697.pdf},
year = {2011}
}
@article{McCullagh2002a,
abstract = {This paper addresses two closely related questions, ``What is a statistical model?'' and ``What is a parameter?'' The notions that a model must ``make sense,'' and that a parameter must ``have a well-defined meaning'' are deeply ingrained in applied statistical work, reasonably well understood at an instinctive level, but absent from most formal theories of modelling and inference. In this paper, these concepts are defined in algebraic terms, using morphisms, functors and natural transformations. It is argued that inference on the basis of a model is not possible unless the model admits a natural extension that includes the domain for which inference is required. For example, prediction requires that the domain include all future units, subjects or time points. Although it is usually not made explicit, every sensible statistical model admits such an extension. Examples are given to show why such an extension is necessary and why a formal theory is required. In the definition of a subparameter, it is shown that certain parameter functions are natural and others are not. Inference is meaningful only for natural parameters. This distinction has important consequences for the construction of prior distributions and also helps to resolve a controversy concerning the Box--Cox model.},
author = {McCullagh, Peter},
doi = {10.1214/aos/1035844977},
isbn = {00905364},
issn = {00905364},
journal = {The Annals of Statistics},
number = {5},
pages = {1225--1310},
title = {{What is a statistical model?}},
volume = {30},
year = {2002}
}
@article{Conference2014,
author = {Conference, Ieee International and Processing, Signal},
file = {:home/dan/edu/mendeley/Chen, Towfic, Sayed - 2014 - Online dictionary learning over distributed models.pdf:pdf},
isbn = {9781479928934},
pages = {3902--3906},
title = {{ONLINE DICTIONARY LEARNING OVER DISTRIBUTED MODELS Jianshu Chen , Zaid J . Towfic , and Ali H . Sayed Department of Electrical Engineering University of California , Los Angeles}},
year = {2014}
}
@article{Hnyk2018,
author = {Hnyk, Daniel},
file = {:home/dan/edu/mendeley/Main(2).pdf:pdf},
title = {{Sequential Dictionary Learning via Nonparametric Bayesian Methods}},
year = {2018}
}
@book{Giudici,
author = {Giudici, Paolo},
file = {:home/dan/edu/mendeley/Giudici - Unknown - Applied Data Mining - Statistical Methods for Business and Industry.pdf:pdf},
isbn = {047084678X},
title = {{Applied Data Mining - Statistical Methods for Business and Industry}}
}
@inproceedings{MairalONDL,
address = {New York, NY, USA},
author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
doi = {10.1145/1553374.1553463},
isbn = {978-1-60558-516-1},
pages = {689--696},
publisher = {ACM},
series = {ICML '09},
title = {{Online Dictionary Learning for Sparse Coding}},
url = {http://doi.acm.org/10.1145/1553374.1553463},
year = {2009}
}
@inproceedings{17_hierarchicalbp_and_ibf,
abstract = {We show that the beta process is the de Finetti mixing distribution underlying the Indian buffet process of [2]. This result shows that the beta process plays the role for the Indian buffet process that the Dirichlet process plays for the Chinese restaurant process, a parallel that guides us in deriving analogs for the beta process of the many known extensions of the Dirichlet process. In particular we define Bayesian hierarchies of beta processes and use the connection to the beta process to develop posterior inference algorithms for the Indian buffet process. We also present an application to document classification, exploring a relationship between the hierarchical beta process and smoothed naive Bayes models.},
address = {San Juan, Puerto Rico},
author = {Thibaux, Romain and Jordan, Michael I},
booktitle = {Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics},
editor = {Meila, Marina and Shen, Xiaotong},
pages = {564--571},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Hierarchical Beta Processes and the Indian Buffet Process}},
url = {http://proceedings.mlr.press/v2/thibaux07a.html},
volume = {2},
year = {2007}
}
@incollection{nonparabayes,
author = {Zhou, Mingyuan and Chen, Haojun and Paisley, John William and Ren, Lu and Sapiro, Guillermo and Carin, Lawrence},
booktitle = {Advances in Neural Information Processing Systems 22},
editor = {Bengio, Y and Schuurmans, D and Lafferty, J and Williams, C and Culotta, A},
file = {:home/dan/edu/mendeley/Zhou - 2012 - Non-Parametric Bayesian Dictionary Learning for Sparse Image Representations (1).pdf:pdf},
pages = {2295--2303},
title = {{Non-Parametric Bayesian Dictionary Learning for Sparse Image Representations.}},
url = {http://books.nips.cc/papers/files/nips22/NIPS2009{\_}0190.pdf},
year = {2009}
}
@article{Liang,
annote = {https://github.com/dawenl/bp{\_}dict{\_}learn/blob/master/bpdl{\_}sampler.py},
author = {Liang, Dawen},
file = {:home/dan/edu/mendeley/Liang - Unknown - Nonparametric Bayesian Dictionary Learning for Machine Listening.pdf:pdf},
title = {{Nonparametric Bayesian Dictionary Learning for Machine Listening}},
url = {http://www.ee.columbia.edu/{~}dliang/files/E6886{\_}sparse.pdf}
}
@article{Teh2008,
author = {Teh, Yee Whye},
file = {:home/dan/edu/mendeley/npbayes.pdf:pdf},
journal = {Slides},
title = {{Bayesian Nonparametric Modelling - slides}},
year = {2008}
}
@misc{Wozniak1999,
author = {Wozniak, Dr. Peter},
file = {:home/dan/edu/mendeley/Wozniak - 1999 - Twenty rules of formulating knowledge.pdf:pdf},
pages = {1--17},
title = {{Twenty rules of formulating knowledge}},
year = {1999}
}
@inbook{jaynes_justice_1986,
author = {Jaynes, E T and Justice, James H},
booktitle = {Maximum Entropy and Bayesian Methods in Applied Statistics: Proceedings of the Fourth Maximum Entropy Workshop University of Calgary, 1984},
doi = {10.1017/CBO9780511569678.003},
pages = {1--25},
publisher = {Cambridge University Press},
title = {{Bayesian Methods: General Background}},
year = {1986}
}
@article{Zhou2012,
abstract = {Nonparametric Bayesian methods are considered for recovery of imagery based upon compressive, incomplete, and/or noisy measurements. A truncated beta-Bernoulli process is employed to infer an appropriate dictionary for the data under test and also for image recovery. In the context of compressive sensing, significant improvements in image recovery are manifested using learned dictionaries, relative to using standard orthonormal image expansions. The compressive-measurement projections are also optimized for the learned dictionary. Additionally, we consider simpler (incomplete) measurements, defined by measuring a subset of image pixels, uniformly selected at random. Spatial interrelationships within imagery are exploited through use of the Dirichlet and probit stick-breaking processes. Several example results are presented, with comparisons to other methods in the literature.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Zhou, Mingyuan and Chen, Haojun and Paisley, John and Ren, Lu and Li, Lingbo and Xing, Zhengming and Dunson, David and Sapiro, Guillermo and Carin, Lawrence},
doi = {10.1109/TIP.2011.2160072},
eprint = {NIHMS150003},
file = {:home/dan/edu/mendeley/Zhou et al. - 2012 - Nonparametric bayesian dictionary learning for analysis of noisy and incomplete images.pdf:pdf},
isbn = {1057-7149},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Bayesian nonparametrics,compressive sensing,dictionary learning,factor analysis,image denoising,image interpolation,sparse coding},
number = {1},
pages = {130--144},
pmid = {21693421},
title = {{Nonparametric bayesian dictionary learning for analysis of noisy and incomplete images}},
volume = {21},
year = {2012}
}
@article{Olshausen1998,
author = {Olshausen, B A and Field, D J},
journal = {Vision Research},
title = {{Sparse coding with an overcomplete basis set: A strategy employed by V1}},
volume = {37},
year = {1998}
}
@book{Bishop2013,
abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. Christopher M. Bishop is Deputy Director of Microsoft Research Cambridge, and holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, a Fellow of the Royal Academy of Engineering, and a Fellow of the Royal Society of Edinburgh. His previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bishop, Christopher M},
booktitle = {Journal of Chemical Information and Modeling},
doi = {10.1117/1.2819119},
eprint = {arXiv:1011.1669v3},
file = {:home/dan/edu/mendeley/Bishop - Pattern Recognition And Machine Learning - Springer 2006(2).pdf:pdf},
isbn = {978-0-387-31073-2},
issn = {1098-6596},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{Pattern Recognition and Machine Learning}},
volume = {53},
year = {2013}
}
@book{Wiley,
author = {Wiley, Jogn},
file = {:home/dan/edu/mendeley/Wiley - Unknown - Clustering.pdf:pdf},
isbn = {9780471719779},
title = {{Clustering}}
}
@article{Paisley2009,
abstract = {We propose a nonparametric extension to the factor analysis problem using a beta process prior. This beta process factor analysis (BP-FA) model allows for a dataset to be decom-posed into a linear combination of a sparse set of factors, providing information on the underlying structure of the observations. As with the Dirichlet process, the beta process is a fully Bayesian conjugate prior, which al-lows for analytical posterior calculation and straightforward inference. We derive a varia-tional Bayes inference algorithm and demon-strate the model on the MNIST digits and HGDP-CEPH cell line panel datasets.},
author = {Paisley, John and Carin, Lawrence},
file = {:home/dan/edu/mendeley/Paisley, Carin - Unknown - Nonparametric Factor Analysis with Beta Process Priors.pdf:pdf},
title = {{Nonparametric Factor Analysis with Beta Process Priors}},
url = {http://www.columbia.edu/{~}jwp2128/Papers/PaisleyCarin2009b.pdf},
year = {2009}
}
@article{frequency_prob,
author = {Friedman, Charles},
doi = {https://doi.org/10.1006/aama.1999.0653},
issn = {0196-8858},
journal = {Advances in Applied Mathematics},
number = {3},
pages = {234--254},
title = {{The Frequency Interpretation in Probability}},
url = {http://www.sciencedirect.com/science/article/pii/S019688589990653X},
volume = {23},
year = {1999}
}
@article{coxfreq,
author = {Cox, R T},
doi = {10.1119/1.1990764},
journal = {American Journal of Physics},
number = {1},
pages = {1--13},
title = {{Probability, Frequency and Reasonable Expectation}},
url = {https://doi.org/10.1119/1.1990764},
volume = {14},
year = {1946}
}
@article{Conference2014a,
archivePrefix = {arXiv},
arxivId = {arXiv:1402.1515v1},
author = {Conference, Ieee International and Processing, Signal and Chen, Jianshu and Towfic, Zaid J. and Sayed, Ali H.},
doi = {10.1109/ICASSP.2014.6854327},
eprint = {arXiv:1402.1515v1},
file = {:home/dan/edu/mendeley/Chen, Towfic, Sayed - 2014 - Online dictionary learning over distributed models.pdf:pdf},
isbn = {9781479928934},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Dictionary learning,diffusion strategies,distributed model,dual decomposition},
pages = {3902--3906},
title = {{Online dictionary learning over distributed models}},
year = {2014}
}
@article{Thibaux2007,
abstract = {We show that the beta process is the de$\backslash$n$\backslash$nFinetti mixing distribution underlying the In-$\backslash$n$\backslash$ndian buffet process of [2]. This result shows$\backslash$n$\backslash$nthat the beta process plays the role for the$\backslash$n$\backslash$nIndian buffet process that the Dirichlet pro-$\backslash$n$\backslash$ncess plays for the Chinese restaurant process,$\backslash$n$\backslash$na parallel that guides us in deriving analogs$\backslash$n$\backslash$nfor the beta process of the many known ex-$\backslash$n$\backslash$ntensions of the Dirichlet process. In partic-$\backslash$n$\backslash$nular we define Bayesian hierarchies of beta$\backslash$n$\backslash$nprocesses and use the connection to the beta$\backslash$n$\backslash$nprocess to develop posterior inference algo-$\backslash$n$\backslash$nrithms for the Indian buffet process. We also$\backslash$n$\backslash$npresent an application to document classifi-$\backslash$n$\backslash$ncation, exploring a relationship between the$\backslash$n$\backslash$nhierarchical beta process and smoothed naive$\backslash$n$\backslash$nBayes models.},
author = {Thibaux, R and Jordan, M I},
doi = {10.1.1.121.455},
file = {:home/dan/edu/mendeley/17{\_}hiearchbp{\_}and{\_}ibf.pdf:pdf},
issn = {15324435},
journal = {Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics (AISTATS)},
keywords = {Beta process,Chinese restaurant process,Dirichlet process,Indian buffet process},
title = {{Hierarchical Beta Processes and the Indian Buffet Process}},
year = {2007}
}
@book{Bishop:2006:PRM:1162264,
address = {Secaucus, NJ, USA},
author = {Bishop, Christopher M},
isbn = {0387310738},
publisher = {Springer-Verlag New York, Inc.},
title = {{Pattern Recognition and Machine Learning (Information Science and Statistics)}},
year = {2006}
}
@inbook{interpretations,
author = {ed {H{\'{a}}jek Zalta}, Edward N},
booktitle = {The Stanford Encyclopedia of Philosophy},
isbn = {1095-5054},
title = {{Interpretations of Probability}},
url = {https://plato.stanford.edu/},
year = {2012}
}
@article{LessWrong,
author = {LessWrong},
file = {:home/dan/edu/mendeley/LessWrong - Unknown - Mysterious Answers to Mysterious Questions.pdf:pdf},
title = {{Mysterious Answers to Mysterious Questions}}
}
@book{Kroese2014,
abstract = {{\textcopyright} The Author(s) 2014. This textbook on statistical modeling and statistical inference will assist advanced undergraduate and graduate students. Statistical Modeling and Computation provides a unique introduction to modern Statistics from both classical and Bayesian perspectives. It also offers an integrated treatment of Mathematical Statistics and modern statistical computation, emphasizing statistical modeling, computational techniques, and applications. Each of the three parts will cover topics essential to university courses. Part I covers the fundamentals of probability theory. In Part II, the authors introduce a wide variety of classical models that include, among others, linear regression and ANOVA models. In Part III, the authors address the statistical analysis and computation of various advanced models, such as generalized linear, state-space and Gaussian models. Particular attention is paid to fast Monte Carlo techniques for Bayesian inference on these models. Throughout the book the authors include a large number of illustrative examples and solved problems. The book also features a section with solutions, an appendix that serves as a MATLAB primer, and a mathematical supplement.},
author = {Kroese, Dirk P. and Chan, Joshua C.C.},
booktitle = {Statistical Modeling and Computation},
doi = {10.1007/978-1-4614-8775-3},
isbn = {9781461487753},
pages = {1--400},
title = {{Statistical modeling and computation}},
year = {2014}
}
