Automatically generated by Mendeley Desktop 1.17.11
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Li1994,
author = {Li, X.R. and Bar-Shalom, Y.},
doi = {10.1109/7.303738},
issn = {00189251},
journal = {IEEE Transactions on Aerospace and Electronic Systems},
month = {jul},
number = {3},
pages = {671--684},
title = {{A recursive multiple model approach to noise identification}},
url = {http://ieeexplore.ieee.org/document/303738/},
volume = {30},
year = {1994}
}
@article{Dupre2004,
abstract = {By basing Bayesian probability theory on five axioms, we can give a trivial proof of Cox's Theorem on the product rule and sum rule for conditional plausibility without assuming continuity or differentiablity of plausibility. Instead, we extend the notion of plausibility to apply to unknowns, giving them plausi-ble values. Thus, we combine the best aspects of two approaches to Bayesian probability theory, namely the Cox-Jaynes theory and the de Finetti theory.},
author = {Dupr{\'{e}}, Maurice J and Tipler, Frank J},
file = {:home/dan/edu/mendeley/Dupr{\'{e}}, Tipler - 2004 - New Axioms for Rigorous Bayesian Probability.pdf:pdf},
journal = {Bayesian Analysis},
keywords = {Axiomatic Bayesian Probability,Cox,Jaynes,Product Rule,Sum Rule,de Finetti},
number = {1},
pages = {1--8},
title = {{New Axioms for Rigorous Bayesian Probability}},
url = {http://dauns.math.tulane.edu/{~}dupre/OLDPUBLIC/20090804 BA FNL 3.pdf},
volume = {1},
year = {2004}
}
@article{Cattivelli2010,
author = {Cattivelli, Federico S. and Sayed, Ali H.},
doi = {10.1109/TAC.2010.2042987},
issn = {0018-9286},
journal = {IEEE Transactions on Automatic Control},
month = {sep},
number = {9},
pages = {2069--2084},
title = {{Diffusion Strategies for Distributed Kalman Filtering and Smoothing}},
url = {http://ieeexplore.ieee.org/document/5411741/},
volume = {55},
year = {2010}
}
@article{Zhou2012,
abstract = {Nonparametric Bayesian methods are considered for recovery of imagery based upon compressive, incomplete, and/or noisy measurements. A truncated beta-Bernoulli process is employed to infer an appropriate dictionary for the data under test and also for image recovery. In the context of compressive sensing, significant improvements in image recovery are manifested using learned dictionaries, relative to using standard orthonormal image expansions. The compressive-measurement projections are also optimized for the learned dictionary. Additionally, we consider simpler (incomplete) measurements, defined by measuring a subset of image pixels, uniformly selected at random. Spatial interrelationships within imagery are exploited through use of the Dirichlet and probit stick-breaking processes. Several example results are presented, with comparisons to other methods in the literature.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Zhou, Mingyuan and Chen, Haojun and Paisley, John and Ren, Lu and Li, Lingbo and Xing, Zhengming and Dunson, David and Sapiro, Guillermo and Carin, Lawrence},
doi = {10.1109/TIP.2011.2160072},
eprint = {NIHMS150003},
file = {:home/dan/edu/mendeley/Zhou et al. - 2012 - Nonparametric bayesian dictionary learning for analysis of noisy and incomplete images.pdf:pdf},
isbn = {1057-7149},
issn = {10577149},
journal = {IEEE Transactions on Image Processing},
keywords = {Bayesian nonparametrics,compressive sensing,dictionary learning,factor analysis,image denoising,image interpolation,sparse coding},
number = {1},
pages = {130--144},
pmid = {21693421},
title = {{Nonparametric bayesian dictionary learning for analysis of noisy and incomplete images}},
volume = {21},
year = {2012}
}
@article{Paisley2009,
abstract = {We propose a nonparametric extension to the factor analysis problem using a beta process prior. This beta process factor analysis (BP-FA) model allows for a dataset to be decom-posed into a linear combination of a sparse set of factors, providing information on the underlying structure of the observations. As with the Dirichlet process, the beta process is a fully Bayesian conjugate prior, which al-lows for analytical posterior calculation and straightforward inference. We derive a varia-tional Bayes inference algorithm and demon-strate the model on the MNIST digits and HGDP-CEPH cell line panel datasets.},
author = {Paisley, John and Carin, Lawrence},
file = {:home/dan/edu/mendeley/Paisley, Carin - Unknown - Nonparametric Factor Analysis with Beta Process Priors.pdf:pdf},
title = {{Nonparametric Factor Analysis with Beta Process Priors}},
url = {http://www.columbia.edu/{~}jwp2128/Papers/PaisleyCarin2009b.pdf},
year = {2009}
}
@inbook{interpretations,
author = {ed {H{\'{a}}jek Zalta}, Edward N},
booktitle = {The Stanford Encyclopedia of Philosophy},
isbn = {1095-5054},
title = {{Interpretations of Probability}},
url = {https://plato.stanford.edu/},
year = {2012}
}
@misc{Wozniak1999,
author = {Wozniak, Dr. Peter},
file = {:home/dan/edu/mendeley/Wozniak - 1999 - Twenty rules of formulating knowledge.pdf:pdf},
pages = {1--17},
title = {{Twenty rules of formulating knowledge}},
year = {1999}
}
@article{Olfati-Saber,
abstract = {The problem of distributed Kalman filtering (DKF) for sensor networks is one of the most fundamental distributed estimation problems for scalable sensor fusion. This paper addresses the DKF problem by reducing it to two separate dy-namic consensus problems in terms of weighted measurements and inverse-covariance matrices. These to data fusion problems are solved is a distributed way using low-pass and band-pass consensus filters. Consensus filters are distributed algorithms that allow calculation of average-consensus of time-varying signals. The stability properties of consensus filters is discussed in a companion CDC '05 paper [24]. We show that a central Kalman filter for sensor networks can be decomposed into n micro-Kalman filters with inputs that are provided by two types of consensus fil-ters. This network of micro-Kalman filters collectively are capable to provide an estimate of the state of the process (under observation) that is identical to the esti-mate obtained by a central Kalman filter given that all nodes agree on two central sums. Later, we demonstrate that our consensus filters can approximate these sums and that gives an approximate distributed Kalman filtering algorithm. A detailed account of the computational and communication architecture of the algorithm is provided. Simulation results are presented for a sensor network with 200 nodes and more than 1000 links.},
author = {Olfati-Saber, Reza},
file = {:home/dan/edu/mendeley/Olfati-Saber - Unknown - Distributed Kalman Filtering and Sensor Fusion in Sensor Networks.pdf:pdf},
keywords = {consensus filters,distributed Kalman filters,micro-Kalman filters,network embedded systems,sensor fusion,sensor networks},
title = {{Distributed Kalman Filtering and Sensor Fusion in Sensor Networks}},
url = {https://pdfs.semanticscholar.org/fd77/043a3e588e03028589615b51f51060074a5b.pdf}
}
@article{E.T.Jaynes1986,
abstract = {We note the main points of history, as a framework on which to hang many background remarks concerning the nature and motivation of Bayesian/Maximum Entropy methods. Experience has shown that these are needed in order to understand recentwork and problems. A more complete account of the history, with many more details and references, is given in Jaynes (1978). The following discussion is essentially nontechnical; the aim is only to convey a little introductory $\backslash$feel" for our outlook, purpose, and terminology, and to alert newcomers to common pitfalls of misunderstanding.},
author = {{E.T. Jaynes}},
doi = {10.1017/CBO9780511569678},
file = {:home/dan/edu/mendeley/general.background (1).pdf:pdf},
isbn = {9780511569678},
journal = {Maximum Entropy and Bayesian Methods in Applied Statistics},
keywords = {Ars Conjectandi,Barnard,Bayes' theorem,Clausius entropy,Cox,Cramer,Darwin,Desideratum of Consistency,Fowler,Gibbs,Gull,Herodotus,Jaynes,Johnson,Jupiter,Kolmogorov,Lindley,Liouville,Mathematical Methods,Poincare,Rissanen,Saturn,Shannon entropy,Shore,Skilling,Sommerfeld,Statistics for physical sciences and engineering,Tower of Babel,Tukey,Twenty Questions,Wald,crystallography,difference,digital,entropy,filters,finite,likelihood,standard deviation,time series},
number = {August 1984},
pages = {1--25},
title = {{Bayesian Methods: General Background}},
url = {/chapter.jsf?bid=CBO9780511569678A007{\&}cid=CBO9780511569678A007},
year = {1986}
}
@inbook{jaynes_justice_1986,
author = {Jaynes, E T and Justice, James H},
booktitle = {Maximum Entropy and Bayesian Methods in Applied Statistics: Proceedings of the Fourth Maximum Entropy Workshop University of Calgary, 1984},
doi = {10.1017/CBO9780511569678.003},
pages = {1--25},
publisher = {Cambridge University Press},
title = {{Bayesian Methods: General Background}},
year = {1986}
}
@article{Sayed2013,
abstract = {Adaptive networks are well-suited to perform decentralized information processing and optimization tasks and to model various types of self-organized and complex behavior encountered in nature. Adaptive networks consist of a collection of agents with processing and learning abilities. The agents are linked together through a connection topology, and they cooperate with each other through local interactions to solve distributed optimization, estimation, and inference problems in real-time. The continuous diffusion of information across the network enables agents to adapt their performance in relation to streaming data and network conditions; it also results in improved adaptation and learning performance relative to non-cooperative agents. This article provides an overview of diffusion strategies for adaptation and learning over networks. The article is divided into several sections:},
archivePrefix = {arXiv},
arxivId = {arXiv:1205.4220v2},
author = {Sayed, Ali H},
eprint = {arXiv:1205.4220v2},
file = {:home/dan/edu/mendeley/Sayed - 2013 - arXiv1205.4220v2 cs.MA 5 May 2013.pdf:pdf},
title = {{Diffusion adaptation over networks}},
url = {https://arxiv.org/pdf/1205.4220.pdf},
year = {2013}
}
@book{Bishop:2006:PRM:1162264,
address = {Secaucus, NJ, USA},
author = {Bishop, Christopher M},
isbn = {0387310738},
publisher = {Springer-Verlag New York, Inc.},
title = {{Pattern Recognition and Machine Learning (Information Science and Statistics)}},
year = {2006}
}
@article{Jaynes2003,
abstract = {The standard rules of probability can be interpreted as uniquely valid principles in logic. In this book, E. T. Jaynes dispels the imaginary distinction between 'probability theory' and 'statistical inference', leaving a logical unity and simplicity, which provides greater technical power and flexibility in applications. This book goes beyond the conventional mathematics of probability theory, viewing the subject in a wider context. New results are discussed, along with applications of probability theory to a wide variety of problems in physics, mathematics, economics, chemistry and biology. It contains many exercises and problems, and is suitable for use as a textbook on graduate level courses involving data analysis. The material is aimed at readers who are already familiar with applied mathematics at an advanced undergraduate level or higher. The book will be of interest to scientists working in any area where inference from incomplete information is necessary.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Jaynes, E. T.},
doi = {10.1007/BF02985800},
eprint = {arXiv:1011.1669v3},
isbn = {0521592712},
issn = {0343-6993},
journal = {The Mathematical Intelligencer},
number = {2},
pages = {83--83},
pmid = {4362089},
title = {{Probability Theory: The Logic of Science.}},
volume = {27},
year = {2003}
}
@article{Mehra1972,
author = {Mehra, R.},
doi = {10.1109/TAC.1972.1100100},
issn = {0018-9286},
journal = {IEEE Transactions on Automatic Control},
month = {oct},
number = {5},
pages = {693--698},
title = {{Approaches to adaptive filtering}},
url = {http://ieeexplore.ieee.org/document/1100100/},
volume = {17},
year = {1972}
}
@book{Bishop2013,
abstract = {The dramatic growth in practical applications for machine learning over the last ten years has been accompanied by many important developments in the underlying algorithms and techniques. For example, Bayesian methods have grown from a specialist niche to become mainstream, while graphical models have emerged as a general framework for describing and applying probabilistic techniques. The practical applicability of Bayesian methods has been greatly enhanced by the development of a range of approximate inference algorithms such as variational Bayes and expectation propagation, while new models based on kernels have had a significant impact on both algorithms and applications. This completely new textbook reflects these recent developments while providing a comprehensive introduction to the fields of pattern recognition and machine learning. It is aimed at advanced undergraduates or first-year PhD students, as well as researchers and practitioners. No previous knowledge of pattern recognition or machine learning concepts is assumed. Familiarity with multivariate calculus and basic linear algebra is required, and some experience in the use of probabilities would be helpful though not essential as the book includes a self-contained introduction to basic probability theory. The book is suitable for courses on machine learning, statistics, computer science, signal processing, computer vision, data mining, and bioinformatics. Extensive support is provided for course instructors, including more than 400 exercises, graded according to difficulty. Example solutions for a subset of the exercises are available from the book web site, while solutions for the remainder can be obtained by instructors from the publisher. The book is supported by a great deal of additional material, and the reader is encouraged to visit the book web site for the latest information. Christopher M. Bishop is Deputy Director of Microsoft Research Cambridge, and holds a Chair in Computer Science at the University of Edinburgh. He is a Fellow of Darwin College Cambridge, a Fellow of the Royal Academy of Engineering, and a Fellow of the Royal Society of Edinburgh. His previous textbook "Neural Networks for Pattern Recognition" has been widely adopted.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bishop, Christopher M},
booktitle = {Journal of Chemical Information and Modeling},
doi = {10.1117/1.2819119},
eprint = {arXiv:1011.1669v3},
file = {:home/dan/edu/mendeley/Bishop - Pattern Recognition And Machine Learning - Springer 2006(2).pdf:pdf},
isbn = {978-0-387-31073-2},
issn = {1098-6596},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{Pattern Recognition and Machine Learning}},
volume = {53},
year = {2013}
}
@article{Karasalo2011,
abstract = {In this paper, an optimization-based adaptive Kalman filtering method is proposed. The method produces an estimate of the process noise covariance matrix Q by solving an optimization problem over a short window of data. The algorithm recovers the observations h(x) from a system ẋ=f(x),y=h(x)+v without a priori knowledge of system dynamics. Potential applications include target tracking using a network of nonlinear sensors, servoing, mapping, and localization. The algorithm is demonstrated in simulations on a tracking example for a target with coupled and nonlinear kinematics. Simulations indicate superiority over a standard MMAE algorithm for a large class of systems.},
author = {Karasalo, Maja and Hu, Xiaoming},
doi = {10.1016/J.AUTOMATICA.2011.04.004},
issn = {0005-1098},
journal = {Automatica},
month = {aug},
number = {8},
pages = {1785--1793},
publisher = {Pergamon},
title = {{An optimization approach to adaptive Kalman filtering}},
url = {https://www.sciencedirect.com/science/article/pii/S000510981100224X},
volume = {47},
year = {2011}
}
@inproceedings{Gaylor2003,
address = {Reston, Virigina},
author = {Gaylor, David and Lightsey, E. Glenn},
booktitle = {AIAA Guidance, Navigation, and Control Conference and Exhibit},
doi = {10.2514/6.2003-5445},
isbn = {978-1-62410-090-1},
month = {aug},
publisher = {American Institute of Aeronautics and Astronautics},
title = {{GPS/INS Kalman Filter Design for Spacecraft Operating in the Proximity of International Space Station}},
url = {http://arc.aiaa.org/doi/10.2514/6.2003-5445},
year = {2003}
}
@article{LessWrong2008,
annote = {Chyb{\'{i}} jak he Simple Truth tak An Intuitive Explanation of Bayes' Theorem},
author = {LessWrong},
file = {:home/dan/edu/mendeley/LessWrong - 2008 - Map and Territory.pdf:pdf},
title = {{Map and Territory}},
year = {2008}
}
@article{McCullagh2002a,
abstract = {This paper addresses two closely related questions, ``What is a statistical model?'' and ``What is a parameter?'' The notions that a model must ``make sense,'' and that a parameter must ``have a well-defined meaning'' are deeply ingrained in applied statistical work, reasonably well understood at an instinctive level, but absent from most formal theories of modelling and inference. In this paper, these concepts are defined in algebraic terms, using morphisms, functors and natural transformations. It is argued that inference on the basis of a model is not possible unless the model admits a natural extension that includes the domain for which inference is required. For example, prediction requires that the domain include all future units, subjects or time points. Although it is usually not made explicit, every sensible statistical model admits such an extension. Examples are given to show why such an extension is necessary and why a formal theory is required. In the definition of a subparameter, it is shown that certain parameter functions are natural and others are not. Inference is meaningful only for natural parameters. This distinction has important consequences for the construction of prior distributions and also helps to resolve a controversy concerning the Box--Cox model.},
author = {McCullagh, Peter},
doi = {10.1214/aos/1035844977},
isbn = {00905364},
issn = {00905364},
journal = {The Annals of Statistics},
number = {5},
pages = {1225--1310},
title = {{What is a statistical model?}},
volume = {30},
year = {2002}
}
@article{SageA.P.;Husa1969,
abstract = {Two problems which occur when methods of optimal estimation are applied to an actual problem are the choice of prior statistics and the choice of a mathematical system model. This paper presents the development of optimal and suboptimal adaptive Bayes estimation algorithms for filtering with unknown prior statistics. Examples illustrate the efficacy of the method. (Author)
},
author = {{Sage, A. P.; Husa}, G. W.},
title = {{Adaptive Filtering with Unknown Prior Statistics. | National Technical Reports Library - NTIS}},
year = {1969}
}
@incollection{nonparabayes,
author = {Zhou, Mingyuan and Chen, Haojun and Paisley, John William and Ren, Lu and Sapiro, Guillermo and Carin, Lawrence and Paisley, John William},
booktitle = {Advances in Neural Information Processing Systems 22},
editor = {Bengio, Y and Schuurmans, D and Lafferty, J and Williams, C and Culotta, A},
file = {:home/dan/edu/mendeley/Mingyuan{\_}nips2009{\_}final.pdf:pdf},
pages = {2295--2303},
title = {{Non-Parametric Bayesian Dictionary Learning for Sparse Image Representations}},
url = {http://books.nips.cc/papers/files/nips22/NIPS2009{\_}0190.pdf},
year = {2009}
}
@article{Mahmoud2013,
abstract = {In recent years, a compelling need has arisen to understand the effects of distributed information structures on estimation and filtering. In this paper, a bibliographical review on distributed Kalman filtering (DKF) is provided. The paper contains a classification of different approaches and methods involved to DKF. The applications of DKF are also discussed and explained separately. A comparison of different approaches is briefly carried out. Focuses on the contemporary research are also addressed with emphasis on the practical applications of the techniques. An exhaustive list of publications, linked directly or indirectly to DKF in the open literature, is compiled to provide an overall picture of different developing aspects of this area. Keywords: Distributed Kalman filtering (DKF), Self-tuning (ST) distributed fusion Kalman filter, distributed particle filtering (DPF), distributed consensus (DC)-based estimation, track-to-track fusion, distributed networks (DN), multi-sensor data fusion systems (MSDF), distributed out-of-sequence measurements (OOSM), diffusion-based DKF.},
author = {Mahmoud, Magdi S and Khalid, Haris M},
file = {:home/dan/edu/mendeley/Mahmoud, Khalid - 2013 - Bibliographic Review on Distributed Kalman Filtering.pdf:pdf},
title = {{Bibliographic Review on Distributed Kalman Filtering}},
url = {http://cogprints.org/8906/1/MsM-KFUPM-DCC-2D-One{\%}5B3R{\%}5D.pdf},
year = {2013}
}
@book{Akyildiz2010,
abstract = {This book presents an in-depth study on the recent advances in Wireless Sensor Networks (WSNs). The authors describe the existing WSN applications and discuss the research efforts being undertaken in this field. Theoretical analysis and factors influencing protocol design are also highlighted. The authors explore state-of-the-art protocols for WSN protocol stack in transport, routing, data link, and physical layers. Moreover, the synchronization and localization problems in WSNs are investigated along with existing solutions. Furthermore, cross-layer solutions are described. Finally, developing areas of WSNs including sensor-actor networks, multimedia sensor networks, and WSN applications in underwater and underground environments are explored. The book is written in an accessible, textbook style, and includes problems and solutions to assist learning. Key Features: The ultimate guide to recent advances and research into WSNs Discusses the most important problems and issues that arise when programming and designing WSN systems Shows why the unique features of WSNs – self-organization, cooperation, correlation -- will enable new applications that will provide the end user with intelligence and a better understanding of the environment Provides an overview of the existing evaluation approaches for WSNs including physical testbeds and software simulation environments Includes examples and learning exercises with a solutions manual; supplemented by an accompanying website containing PPT-slides. Wireless Sensor Networks is an essential textbook for advanced students on courses in wireless communications, networking and computer science. It will also be of interest to researchers, system and chip designers, network planners, technical mangers and other professionals in these fields.},
author = {Akyildiz, Ian F. and Vuran, Mehmet Can},
booktitle = {Zhurnal Eksperimental'noi i Teoreticheskoi Fiziki},
doi = {10.1002/9780470515181},
isbn = {9780470515181},
pages = {516},
title = {{Wireless Sensor Networks}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:No+Title{\#}0{\%}5Cnhttp://doi.wiley.com/10.1002/9780470515181},
year = {2010}
}
@book{Parisi.1988,
author = {Parisi, Giorgio},
title = {{Statistical Field Theory}},
year = {1988}
}
@article{Edwards,
author = {Edwards, Paul N},
file = {:home/dan/edu/mendeley/Edwards - Unknown - How to Read a Book, v5.0.pdf:pdf},
pages = {0--10},
title = {{How to Read a Book, v5.0}}
}
@article{Conference2014,
author = {Conference, Ieee International and Processing, Signal},
file = {:home/dan/edu/mendeley/Chen, Towfic, Sayed - 2014 - Online dictionary learning over distributed models.pdf:pdf},
isbn = {9781479928934},
pages = {3902--3906},
title = {{ONLINE DICTIONARY LEARNING OVER DISTRIBUTED MODELS Jianshu Chen , Zaid J . Towfic , and Ali H . Sayed Department of Electrical Engineering University of California , Los Angeles}},
year = {2014}
}
@article{Wikipediaa,
author = {Wikipedia, From},
file = {:home/dan/edu/mendeley/kapitola2(2).pdf:pdf},
pages = {1--5},
title = {{Sparse dictionary learning}}
}
@inproceedings{MairalONDL,
abstract = {Sparse coding—that is, modelling data vectors as sparse linear combinations of basis elements—is widely used in machine learning, neuroscience, signal processing, and statistics. This paper fo- cuses on learning the basis set, also called dic- tionary, to adapt it to specific data, an approach that has recently proven to be very effective for signal reconstruction and classification in the au- dio and image processing domains. This paper proposes a new online optimization algorithm for dictionary learning, based on stochastic ap- proximations, which scales up gracefully to large datasets with millions of training samples. A proof of convergence is presented, along with experiments with natural images demonstrating that it leads to faster performance and better dic- tionaries than classical batch algorithms for both small and large datasets.},
address = {New York, NY, USA},
archivePrefix = {arXiv},
arxivId = {0908.0050},
author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning - ICML '09},
doi = {10.1145/1553374.1553463},
eprint = {0908.0050},
file = {:home/dan/edu/mendeley/Mairal et al. - Unknown - Online Dictionary Learning for Sparse Coding.pdf:pdf},
isbn = {9781605585161},
issn = {0016450X},
pages = {1--8},
pmid = {710806},
publisher = {ACM},
series = {ICML '09},
title = {{Online dictionary learning for sparse coding}},
url = {http://portal.acm.org/citation.cfm?doid=1553374.1553463},
year = {2009}
}
@article{applied_dec,
author = {Raiffa, Howard and Schlaifer, Robert},
doi = {10.1002/bs.3830070108},
issn = {1099-1743},
journal = {Behavioral Science},
number = {1},
pages = {103--104},
publisher = {John Wiley {\&} Sons, Ltd.},
title = {{Applied statistical decision theory}},
url = {http://dx.doi.org/10.1002/bs.3830070108},
volume = {7},
year = {1962}
}
@article{frequency_prob,
author = {Friedman, Charles},
doi = {https://doi.org/10.1006/aama.1999.0653},
issn = {0196-8858},
journal = {Advances in Applied Mathematics},
number = {3},
pages = {234--254},
title = {{The Frequency Interpretation in Probability}},
url = {http://www.sciencedirect.com/science/article/pii/S019688589990653X},
volume = {23},
year = {1999}
}
@article{Pasricha2006,
abstract = {The paper is an eclectic study of the uses of the Kalman filter in existing econometric literature. An effort is made to introduce the various extensions to the linear filter first developed by Kalman(1960) through examples of their uses in economics. The basic filter is first derived and then some applications are reviewed.},
author = {Pasricha, Gurnain Kaur},
file = {:home/dan/edu/mendeley/Pasricha - 2006 - M P RA Kalman Filter and its Economic Applications Kalman Filter and its Economic Applications.pdf:pdf},
keywords = {Kalman Filter,Markov Switching,Stochastic Volatility,Time-varying Parameters},
title = {{M P RA Kalman Filter and its Economic Applications Kalman Filter and its Economic Applications}},
url = {http://mpra.ub.uni-muenchen.de/22734/},
year = {2006}
}
@article{Griffiths2011,
abstract = {The Indian buffet process is a stochastic process defining a probability distribution over equiva- lence classes of sparse binary matrices with a finite number of rows and an unbounded number of columns. This distribution is suitable for use as a prior in probabilisticmodels that represent objects using a potentially infinite array of features, or that involve bipartite graphs in which the size of at least one class of nodes is unknown. We give a detailed derivation of this distribution, and illustrate its use as a prior in an infinite latent featuremodel. We then review recent applications of the Indian buffet process in machine learning, discuss its extensions, and summarize its connections to other stochastic processes.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Griffiths, Thomas L. and Ghahramani, Zoubin},
doi = {10.1016/j.biotechadv.2011.08.021.Secreted},
eprint = {NIHMS150003},
file = {:home/dan/edu/mendeley/ibf{\_}review.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal ofMachine Learning Research},
keywords = {beta process,chinese,exchangeable distributions,latent variable models,markov chain monte carlo,nonparametric bayes,restaurant processes,sparse binary matrices},
pages = {1185--1224},
pmid = {290096100001},
title = {{The Indian Buffet Process: An Introduction and Review}},
volume = {12},
year = {2011}
}
@incollection{nonparabayes,
author = {Zhou, Mingyuan and Chen, Haojun and Paisley, John William and Ren, Lu and Sapiro, Guillermo and Carin, Lawrence},
booktitle = {Advances in Neural Information Processing Systems 22},
editor = {Bengio, Y and Schuurmans, D and Lafferty, J and Williams, C and Culotta, A},
file = {:home/dan/edu/mendeley/Zhou - 2012 - Non-Parametric Bayesian Dictionary Learning for Sparse Image Representations (1).pdf:pdf},
pages = {2295--2303},
title = {{Non-Parametric Bayesian Dictionary Learning for Sparse Image Representations.}},
url = {http://books.nips.cc/papers/files/nips22/NIPS2009{\_}0190.pdf},
year = {2009}
}
@article{Jolliffe2002,
abstract = {Timmerman reviews Principal Component Analysis (2nd Ed.). I. T. Jolliffe},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Jolliffe, I T},
doi = {10.1007/b98835},
eprint = {arXiv:1011.1669v3},
isbn = {0-387-95442-2},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {principal component analysis,statistical theory methods},
pages = {487},
pmid = {21674720},
title = {{Principal Component Analysis}},
url = {http://link.springer.com/10.1007/b98835},
volume = {98},
year = {2002}
}
@article{Posterior,
author = {Posterior, Prior and Conjugate, Likelihood and Principle, Hyperparameter Hyperprior and Bayes, Empirical and Bayesian, Approximate and Bayes, Naive},
file = {:home/dan/edu/mendeley/kapitola1(2).pdf:pdf},
number = {I},
pages = {1--8},
title = {{Bayesian probability}},
volume = {1}
}
@article{Gao2012,
author = {Gao, Xiangdong and You, Deyong and Katayama, Seiji},
doi = {10.1109/TIE.2012.2193854},
issn = {0278-0046},
journal = {IEEE Transactions on Industrial Electronics},
month = {nov},
number = {11},
pages = {4315--4325},
title = {{Seam Tracking Monitoring Based on Adaptive Kalman Filter Embedded Elman Neural Network During High-Power Fiber Laser Welding}},
url = {http://ieeexplore.ieee.org/document/6179988/},
volume = {59},
year = {2012}
}
@article{LessWrong,
author = {LessWrong},
file = {:home/dan/edu/mendeley/LessWrong - Unknown - Mysterious Answers to Mysterious Questions.pdf:pdf},
title = {{Mysterious Answers to Mysterious Questions}}
}
@inproceedings{Engan1999,
author = {Engan, K. and Aase, S.O. and {Hakon Husoy}, J.},
booktitle = {1999 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings. ICASSP99 (Cat. No.99CH36258)},
doi = {10.1109/ICASSP.1999.760624},
isbn = {0-7803-5041-3},
pages = {2443--2446 vol.5},
publisher = {IEEE},
title = {{Method of optimal directions for frame design}},
url = {http://ieeexplore.ieee.org/document/760624/},
year = {1999}
}
@article{Aharon2003,
abstract = {Modeling signals by sparse and redundant representations has been drawing considerable attention in recent years. Coupled with the ability to train the dictionary using signal examples, these techniques have been shown to lead to state-of-the-art results in a series of recent applications. In this paper we propose a novel structure of such a model for representing image content. The new dictionary is itself a small image, such that every patch in it (in varying location and size) is a possible atom in the representation. We refer to this as the image-signature-dictionary (ISD) and show how it can be trained from image examples. This structure extends the well-known image and video epitomes, as introduced by Jojic, Frey, and Kannan [in pp. 42–49], by replacing a probabilistic averaging of patches with their sparse representations. The ISD enjoys several important features, such as shift and scale flexibilities, and smaller memory and computational requirements, compared to the classical dictionary approach. As a demonstration of these benefits, we present high-quality image denoising results based on this new model.},
author = {Aharon, Michal and Elad, Michael},
doi = {10.1137/07070156X},
file = {:home/dan/edu/mendeley/Aharon, Elad - 2003 - Sparse and Redundant Modeling of Image Content Using an Image-Signature-Dictionary.pdf:pdf},
journal = {Society for Industrial and Applied Mathematics},
keywords = {62H35,MOD AMS subject classifications 68U10,denoising,dictionary,image-signature,learning,matching pursuit,sparse representation},
number = {3},
pages = {228--247},
title = {{Sparse and Redundant Modeling of Image Content Using an Image-Signature-Dictionary *}},
url = {https://epubs.siam.org/doi/pdf/10.1137/07070156X},
volume = {1},
year = {2003}
}
@article{Hartikainen2013,
abstract = {This paper is considered with joint estimation of state and time-varying noise covariance matrices in non-linear stochastic state space models. We present a variational Bayes and Gaussian filtering based algorithm for efficient computation of the approximate filtering posterior distributions. The Gaussian filtering based formulation of the non-linear state space model computation allows usage of efficient Gaussian integration methods such as unscented transform, cubature integration and Gauss-Hermite integration along with the classical Taylor series approximations. The performance of the algorithm is illustrated in a simulated application.},
archivePrefix = {arXiv},
arxivId = {1302.0681},
author = {Hartikainen, Simo S{\"{a}}rkk{\"{a}} Jouni},
eprint = {1302.0681},
file = {:home/dan/edu/mendeley/Hartikainen - 2013 - Variational Bayesian Adaptation of Noise Covariances in Non-Linear Kalman Filtering.pdf:pdf},
month = {feb},
title = {{Variational Bayesian Adaptation of Noise Covariances in Non-Linear Kalman Filtering}},
url = {http://arxiv.org/abs/1302.0681},
year = {2013}
}
@article{Lin1997,
author = {Lin, Shih-Tin},
doi = {10.1023/A:1007946400645},
file = {:home/dan/edu/mendeley/Lin - 1997 - Force Sensing Using Kalman Filtering Techniques for Robot Compliant Motion Control.pdf:pdf},
issn = {09210296},
journal = {Journal of Intelligent and Robotic Systems},
number = {1},
pages = {1--16},
publisher = {Kluwer Academic Publishers},
title = {{Force Sensing Using Kalman Filtering Techniques for Robot Compliant Motion Control}},
url = {http://link.springer.com/10.1023/A:1007946400645},
volume = {18},
year = {1997}
}
@article{Welch2006,
abstract = {In 1960, R.E. Kalman published his famous paper describing a recursive solution to the discrete-data linear filtering problem. Since that time, due in large part to ad-vances in digital computing, the Kalman filter has been the subject of extensive re-search and application, particularly in the area of autonomous or assisted navigation. The Kalman filter is a set of mathematical equations that provides an efficient com-putational (recursive) means to estimate the state of a process, in a way that mini-mizes the mean of the squared error. The filter is very powerful in several aspects: it supports estimations of past, present, and even future states, and it can do so even when the precise nature of the modeled system is unknown.},
author = {Welch, Greg and Bishop, Gary},
file = {:home/dan/edu/mendeley/Welch, Bishop - 2006 - An Introduction to the Kalman Filter.pdf:pdf},
title = {{An Introduction to the Kalman Filter}},
url = {https://www.cs.unc.edu/{~}welch/media/pdf/kalman{\_}intro.pdf},
year = {2006}
}
@article{Dedecius2017,
author = {Dedecius, Kamil and Seckarova, Vladimira},
doi = {10.1109/TSP.2017.2725226},
issn = {1053-587X},
journal = {IEEE Transactions on Signal Processing},
month = {oct},
number = {19},
pages = {5153--5163},
title = {{Factorized Estimation of Partially Shared Parameters in Diffusion Networks}},
url = {http://ieeexplore.ieee.org/document/7973017/},
volume = {65},
year = {2017}
}
@inproceedings{17_hierarchicalbp_and_ibf,
abstract = {We show that the beta process is the de Finetti mixing distribution underlying the Indian buffet process of [2]. This result shows that the beta process plays the role for the Indian buffet process that the Dirichlet process plays for the Chinese restaurant process, a parallel that guides us in deriving analogs for the beta process of the many known extensions of the Dirichlet process. In particular we define Bayesian hierarchies of beta processes and use the connection to the beta process to develop posterior inference algorithms for the Indian buffet process. We also present an application to document classification, exploring a relationship between the hierarchical beta process and smoothed naive Bayes models.},
address = {San Juan, Puerto Rico},
author = {Thibaux, Romain and Jordan, Michael I},
booktitle = {Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics},
editor = {Meila, Marina and Shen, Xiaotong},
file = {:home/dan/edu/mendeley/17{\_}hiearchbp{\_}and{\_}ibf.pdf:pdf},
pages = {564--571},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Hierarchical Beta Processes and the Indian Buffet Process}},
url = {http://proceedings.mlr.press/v2/thibaux07a.html},
volume = {2},
year = {2007}
}
@article{Teh2008,
author = {Teh, Yee Whye},
file = {:home/dan/edu/mendeley/npbayes.pdf:pdf},
journal = {Slides},
title = {{Bayesian Nonparametric Modelling - slides}},
year = {2008}
}
@article{Blei2017,
abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
archivePrefix = {arXiv},
arxivId = {arXiv:1601.00670v6},
author = {Blei, David M and Kucukelbir, Alp and Mcauliffe, Jon D},
eprint = {arXiv:1601.00670v6},
file = {:home/dan/edu/mendeley/Blei, Kucukelbir, Mcauliffe - 2017 - Variational Inference A Review for Statisticians.pdf:pdf},
keywords = {Algorithms,Computationally Intensive Methods,Statistical Computing},
title = {{Variational Inference: A Review for Statisticians}},
url = {https://arxiv.org/pdf/1601.00670.pdf},
year = {2017}
}
@book{Upton2008,
abstract = {This wide-ranging, jargon-free dictionary contains over 2,000 entries on all aspects of statistics including terms used in computing, mathematics, operational research, and probability, as well as biographical information on over 200 key figures in the field, and coverage of statistical journals and societies. It embraces the whole multi-disciplinary spectrum of this complex subject, and will be invaluable for students and professionals from a wide range of disciplines, including politics, market research, medicine, psychology, pharmaceuticals, and mathematics. The entries are generously illustrated with useful figures and diagrams, and include worked examples where applicable, which place them in a practical context. Fully updated for the second edition, the dictionary now boasts over 200 new entries including over 30 new biographies, as well as internet links which point to useful sites for further information, and many additional illustrative examples that clarify terms by showing them in use.},
author = {Upton, G. and Cook, I.},
booktitle = {Oxford university press},
doi = {10.1093/acref/9780199541454.001.0001},
isbn = {978-0-19-954145-4},
pages = {512},
pmid = {15493422},
title = {{A Dictionary of Statistics}},
year = {2008}
}
@article{Aharon2006,
author = {Aharon, M. and Elad, M. and Bruckstein, A.},
doi = {10.1109/TSP.2006.881199},
journal = {IEEE Transactions on Signal Processing},
month = {nov},
number = {11},
pages = {4311--4322},
title = {{K-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation}},
url = {http://ieeexplore.ieee.org/document/1710377/},
volume = {54},
year = {2006}
}
@inproceedings{Hide,
author = {Hide, C. and Moore, T. and Smith, M.},
booktitle = {PLANS 2004. Position Location and Navigation Symposium (IEEE Cat. No.04CH37556)},
doi = {10.1109/PLANS.2004.1308998},
isbn = {0-7803-8416-4},
pages = {227--233},
publisher = {IEEE},
title = {{Adaptive Kalman filtering algorithms for integrating GPS and low cost INS}},
url = {http://ieeexplore.ieee.org/document/1308998/}
}
@article{Winn2005,
abstract = {This paper presents Variational Message Passing (VMP), a general purpose algorithm for applying variational inference to a Bayesian Network. Like belief propagation, Variational Message Passing proceeds by passing messages between nodes in the graph and updating posterior beliefs using local operations at each node. Each such update increases a lower bound on the log evidence (unless already at a local maximum). In contrast to belief propagation, VMP can be applied to a very general class of conjugate-exponential models because it uses a factorised variational approximation. Furthermore, by introducing ad- ditional variational parameters, VMP can be applied to models containing non-conjugate distributions. The VMP framework also allows the lower bound to be evaluated, and this can be used both for model comparison and for detection of convergence. Variational Mes- sage Passing has been implemented in the form of a general purpose inference engine called VIBES (‘Variational Inference for BayEsian networkS') which allows models to be specified graphically and then solved variationally without recourse to coding.},
author = {Winn, John and Bishop, Cm and Jaakkola, T},
doi = {10.1007/s002130100880},
file = {:home/dan/edu/mendeley/Winn, Bishop - 2005 - Variational Message Passing.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {bayesian networks,variational methods},
pages = {661--694},
pmid = {11713616},
title = {{Variational Message Passing}},
volume = {6},
year = {2005}
}
@article{MacQueen1967,
abstract = {Some methods for classification and analysis of multivariate observations},
author = {MacQueen, James},
file = {:home/dan/edu/mendeley/J. MacQueen - 1967 - Some methods for classification and analysis of multivariate observations.pdf:pdf},
journal = {Proceedings of the Berkeley Symposium on Mathematical Statistics and Probability},
pages = {281--297},
publisher = {University of California Press},
title = {{Some methods for classification and analysis of multivariate observations}},
url = {https://projecteuclid.org/euclid.bsmsp/1200512992},
volume = {1},
year = {1967}
}
@book{Kendall2004,
abstract = {6th ed. Includes various eds. of some vols. Vol. 2A (1999) has imprint: London : Arnold ; New York : Oxford University Press. Vol. 2B (2004) has imprint: London : Arnold ; New York : Oxford University Press [distributor]. v. 1. Distribution theory / Alan Stuart, J. Keith Ord -- v. 2A. Classical inference and the linear model / Alan Stuart, J. Keith Ord, Steven Arnold -- v. 2B. Bayesian inference / Anthony O'Hagan.},
author = {Kendall, Maurice G. (Maurice George) and Stuart, Alan and Ord, J. K. and Arnold, Steven F. and O'Hagan, Anthony.},
isbn = {0340807520},
publisher = {Edward Arnold},
title = {{Kendall's advanced theory of statistics.}},
url = {https://eprints.soton.ac.uk/46376/},
year = {2004}
}
@article{Hnyk2018,
author = {Hnyk, Daniel},
file = {:home/dan/edu/mendeley/Main(2).pdf:pdf},
title = {{Sequential Dictionary Learning via Nonparametric Bayesian Methods}},
year = {2018}
}
@book{Giudici,
author = {Giudici, Paolo},
file = {:home/dan/edu/mendeley/Giudici - Unknown - Applied Data Mining - Statistical Methods for Business and Industry.pdf:pdf},
isbn = {047084678X},
title = {{Applied Data Mining - Statistical Methods for Business and Industry}}
}
@article{Strid2009,
author = {Strid, Ingvar and Walentin, Karl},
doi = {10.1007/s10614-008-9160-4},
file = {:home/dan/edu/mendeley/Strid, Walentin - 2009 - Block Kalman Filtering for Large-Scale DSGE Models.pdf:pdf},
issn = {0927-7099},
journal = {Computational Economics},
month = {apr},
number = {3},
pages = {277--304},
publisher = {Springer US},
title = {{Block Kalman Filtering for Large-Scale DSGE Models}},
url = {http://link.springer.com/10.1007/s10614-008-9160-4},
volume = {33},
year = {2009}
}
@book{Wiley,
author = {Wiley, Jogn},
file = {:home/dan/edu/mendeley/Wiley - Unknown - Clustering.pdf:pdf},
isbn = {9780471719779},
title = {{Clustering}}
}
@article{Thibaux2007,
abstract = {We show that the beta process is the de$\backslash$n$\backslash$nFinetti mixing distribution underlying the In-$\backslash$n$\backslash$ndian buffet process of [2]. This result shows$\backslash$n$\backslash$nthat the beta process plays the role for the$\backslash$n$\backslash$nIndian buffet process that the Dirichlet pro-$\backslash$n$\backslash$ncess plays for the Chinese restaurant process,$\backslash$n$\backslash$na parallel that guides us in deriving analogs$\backslash$n$\backslash$nfor the beta process of the many known ex-$\backslash$n$\backslash$ntensions of the Dirichlet process. In partic-$\backslash$n$\backslash$nular we define Bayesian hierarchies of beta$\backslash$n$\backslash$nprocesses and use the connection to the beta$\backslash$n$\backslash$nprocess to develop posterior inference algo-$\backslash$n$\backslash$nrithms for the Indian buffet process. We also$\backslash$n$\backslash$npresent an application to document classifi-$\backslash$n$\backslash$ncation, exploring a relationship between the$\backslash$n$\backslash$nhierarchical beta process and smoothed naive$\backslash$n$\backslash$nBayes models.},
author = {Thibaux, R and Jordan, M I},
doi = {10.1.1.121.455},
file = {:home/dan/edu/mendeley/17{\_}hiearchbp{\_}and{\_}ibf.pdf:pdf},
issn = {15324435},
journal = {Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics (AISTATS)},
keywords = {Beta process,Chinese restaurant process,Dirichlet process,Indian buffet process},
title = {{Hierarchical Beta Processes and the Indian Buffet Process}},
year = {2007}
}
@article{Mairal,
abstract = {Sparse coding—that is, modelling data vectors as sparse linear combinations of basis elements—is widely used in machine learning, neuroscience, signal processing, and statistics. This paper fo-cuses on learning the basis set, also called dic-tionary, to adapt it to specific data, an approach that has recently proven to be very effective for signal reconstruction and classification in the au-dio and image processing domains. This paper proposes a new online optimization algorithm for dictionary learning, based on stochastic ap-proximations, which scales up gracefully to large datasets with millions of training samples. A proof of convergence is presented, along with experiments with natural images demonstrating that it leads to faster performance and better dic-tionaries than classical batch algorithms for both small and large datasets.},
archivePrefix = {arXiv},
arxivId = {0908.0050},
author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo},
doi = {10.1145/1553374.1553463},
eprint = {0908.0050},
file = {:home/dan/edu/mendeley/Mairal et al. - Unknown - Online Dictionary Learning for Sparse Coding.pdf:pdf},
isbn = {9781605585161},
issn = {0016450X},
journal = {Proceedings of the 26th International Conference on Machine Learning},
pmid = {710806},
title = {{Online Dictionary Learning for Sparse Coding}},
url = {http://www.di.ens.fr/sierra/pdfs/icml09.pdf},
year = {2009}
}
@inproceedings{MairalONDL,
address = {New York, NY, USA},
author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
doi = {10.1145/1553374.1553463},
file = {:home/dan/edu/mendeley/Mairal et al. - Unknown - Online Dictionary Learning for Sparse Coding.pdf:pdf},
isbn = {978-1-60558-516-1},
pages = {689--696},
publisher = {ACM},
series = {ICML '09},
title = {{Online Dictionary Learning for Sparse Coding}},
url = {http://doi.acm.org/10.1145/1553374.1553463},
year = {2009}
}
@book{Kroese2014,
abstract = {{\textcopyright} The Author(s) 2014. This textbook on statistical modeling and statistical inference will assist advanced undergraduate and graduate students. Statistical Modeling and Computation provides a unique introduction to modern Statistics from both classical and Bayesian perspectives. It also offers an integrated treatment of Mathematical Statistics and modern statistical computation, emphasizing statistical modeling, computational techniques, and applications. Each of the three parts will cover topics essential to university courses. Part I covers the fundamentals of probability theory. In Part II, the authors introduce a wide variety of classical models that include, among others, linear regression and ANOVA models. In Part III, the authors address the statistical analysis and computation of various advanced models, such as generalized linear, state-space and Gaussian models. Particular attention is paid to fast Monte Carlo techniques for Bayesian inference on these models. Throughout the book the authors include a large number of illustrative examples and solved problems. The book also features a section with solutions, an appendix that serves as a MATLAB primer, and a mathematical supplement.},
author = {Kroese, Dirk P. and Chan, Joshua C.C.},
booktitle = {Statistical Modeling and Computation},
doi = {10.1007/978-1-4614-8775-3},
isbn = {9781461487753},
pages = {1--400},
title = {{Statistical modeling and computation}},
year = {2014}
}
@inproceedings{Stuart1994,
author = {Stuart, A and Ord, K},
title = {{Kendall's Advanced Theory of Statistics: Volume I—Distribution Theory}},
year = {1994}
}
@article{Zhou2009b,
author = {Zhou, Mingyuan and Chen, Haojun and Paisley, John William and Ren, Lu and Sapiro, Guillermo and Carin, Lawrence},
file = {:home/dan/edu/mendeley/Mingyuan{\_}nips2009{\_}final.pdf:pdf},
pages = {2295--2303},
title = {{Non-Parametric {\{}B{\}}ayesian Dictionary Learning for Sparse Image Representations.}},
year = {2009}
}
@article{coxfreq,
author = {Cox, R T},
doi = {10.1119/1.1990764},
journal = {American Journal of Physics},
number = {1},
pages = {1--13},
title = {{Probability, Frequency and Reasonable Expectation}},
url = {https://doi.org/10.1119/1.1990764},
volume = {14},
year = {1946}
}
@article{sparsesolsl1,
author = {Donoho, David L},
doi = {10.1002/cpa.20132},
issn = {1097-0312},
journal = {Communications on Pure and Applied Mathematics},
number = {6},
pages = {797--829},
publisher = {Wiley Subscription Services, Inc., A Wiley Company},
title = {{For most large underdetermined systems of linear equations the minimal l1-norm solution is also the sparsest solution}},
url = {http://dx.doi.org/10.1002/cpa.20132},
volume = {59},
year = {2006}
}
@article{Blei2006,
abstract = {Dirichlet process (DP) mixture models are the cornerstone of nonparametric Bayesian statistics, and the development of Monte-Carlo Markov chain (MCMC) sampling methods for DP mixtures has enabled the application of nonparametric Bayesian methods to a variety of practical data analysis problems. However, MCMC sampling can be prohibitively slow, and it is important to explore alternatives. One class of alternatives is provided by variational methods, a class of deterministic algorithms that convert inference problems into optimization problems (Opper and Saad 2001; Wainwright and Jordan 2003). Thus far, variational methods have mainly been explored in the parametric setting, in particular within the formalism of the exponential family (Attias 2000; Ghahramani and Beal 2001; Blei et al. 2003). In this paper, we present a variational inference algorithm for DP mixtures. We present experiments that compare the algorithm to Gibbs sampling algorithms for DP mixtures of Gaussians and present an application to a large-scale image analysis problem.},
author = {Blei, David M. and Jordan, Michael I.},
doi = {10.1214/06-BA104},
file = {:home/dan/edu/mendeley/Blei, Jordan - 2006 - Variational inference for Dirichlet process mixtures.pdf:pdf},
isbn = {1936-0975},
issn = {19360975},
journal = {Bayesian Analysis},
keywords = {Bayesian computation,Dirichlet processes,Hierarchical models,Image processing,Variational inference},
number = {1 A},
pages = {121--144},
title = {{Variational inference for Dirichlet process mixtures}},
volume = {1},
year = {2006}
}
@article{Wikipedia,
author = {Wikipedia, From},
file = {:home/dan/edu/mendeley/kapitola2.pdf:pdf},
pages = {2--6},
title = {{Sparse dictionary learning}}
}
@article{Wolpert2000,
abstract = {Computational principles of movement neuroscience},
author = {Wolpert, Daniel M. and Ghahramani, Zoubin},
doi = {10.1038/81497},
file = {:home/dan/edu/mendeley/Wolpert, Ghahramani - 2000 - Computational principles of movement neuroscience.pdf:pdf},
issn = {10976256},
journal = {Nature Neuroscience},
month = {nov},
number = {Supp},
pages = {1212--1217},
publisher = {Nature Publishing Group},
title = {{Computational principles of movement neuroscience}},
url = {http://www.nature.com/doifinder/10.1038/81497},
volume = {3},
year = {2000}
}
@misc{LessWrong2014,
author = {LessWrong},
file = {:home/dan/edu/mendeley/LessWrong - 2014 - An abridged introduction to Less Wrong.pdf:pdf},
keywords = {Less Wrong},
title = {{An abridged introduction to Less Wrong}},
year = {2014}
}
@article{Jolliffe2002a,
abstract = {Timmerman reviews Principal Component Analysis (2nd Ed.). I. T. Jolliffe},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Jolliffe, I T},
doi = {10.1007/b98835},
eprint = {arXiv:1011.1669v3},
isbn = {0-387-95442-2},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {principal component analysis,statistical theory methods},
pages = {487},
pmid = {21674720},
title = {{Principal Component Analysis}},
url = {http://link.springer.com/10.1007/b98835},
volume = {98},
year = {2002}
}
@article{Tracy2001,
author = {Tracy, B},
file = {:home/dan/edu/mendeley/Tracy - 2001 - Eat That Frog!.pdf:pdf},
isbn = {1583762027},
pages = {1--113},
title = {{Eat That Frog!}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=A10sRE{\_}q-ScC{\&}oi=fnd{\&}pg=PR9{\&}dq=Eat+That+Frog{\&}ots=x0sIiVN2Dk{\&}sig=iJzHlt85cfpi-HHHVMXqdLnmymA},
year = {2001}
}
@article{Olshausen1998,
author = {Olshausen, B A and Field, D J},
journal = {Vision Research},
title = {{Sparse coding with an overcomplete basis set: A strategy employed by V1}},
volume = {37},
year = {1998}
}
@inproceedings{Olfati-Sabera,
abstract = {— Consensus algorithms for networked dynamic systems provide scalable algorithms for sensor fusion in sensor networks. This paper introduces a distributed filter that allows the nodes of a sensor network to track the average of n sensor measurements using an average consensus based distributed filter called consensus filter. This consensus filter plays a crucial role in solving a data fusion problem that allows implemen-tation of a scheme for distributed Kalman filtering in sensor networks. The analysis of the convergence, noise propagation reduction, and ability to track fast signals are provided for consensus filters. As a byproduct, a novel critical phenomenon is found that relates the size of a sensor network to its tracking and sensor fusion capabilities. We characterize this performance limitation as a tracking uncertainty principle. This answers a fundamental question regarding how large a sensor network must be for effective sensor fusion. Moreover, regular networks emerge as efficient topologies for distributed fusion of noisy information. Though, arbitrary overlay networks can be used. Simulation results are provided that demonstrate the effectiveness of consensus filters for distributed sensor fusion.},
author = {Olfati-Saber, Reza and Shamma, Jeff S},
booktitle = {44th IEEE Conference on Decision and Control, and the European Control Conference},
doi = {10.1109/MCS.2016.2558444},
file = {:home/dan/edu/mendeley/Olfati-Saber, Shamma - Unknown - Consensus Filters for Sensor Networks and Distributed Sensor Fusion.pdf:pdf},
keywords = {complex networks,consensus problems,distributed Kalman filters,graph Laplacians,networked dynamic systems,sensor fusion,sensor networks},
title = {{Consensus Filters for Sensor Networks and Distributed Sensor Fusion}},
url = {http://folk.ntnu.no/skoge/prost/proceedings/cdc-ecc05/pdffiles/papers/1643.pdf},
year = {2005}
}
@book{LessWrong2013,
author = {LessWrong},
file = {:home/dan/edu/mendeley/LessWrong - 2013 - LessWrong.com Sequences.pdf:pdf},
title = {{LessWrong.com Sequences}},
year = {2013}
}
@article{Conference2014a,
archivePrefix = {arXiv},
arxivId = {arXiv:1402.1515v1},
author = {Conference, Ieee International and Processing, Signal and Chen, Jianshu and Towfic, Zaid J. and Sayed, Ali H.},
doi = {10.1109/ICASSP.2014.6854327},
eprint = {arXiv:1402.1515v1},
file = {:home/dan/edu/mendeley/Chen, Towfic, Sayed - 2014 - Online dictionary learning over distributed models.pdf:pdf},
isbn = {9781479928934},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Dictionary learning,diffusion strategies,distributed model,dual decomposition},
pages = {3902--3906},
title = {{Online dictionary learning over distributed models}},
year = {2014}
}
@article{Gershman2011,
abstract = {A key problem in statistical modeling is model selection, how to choose a model at an appropriate level of complexity. This problem appears in many settings, most prominently in choosing the number of clusters in mixture models or the number of factors in factor analysis. In this tutorial we describe Bayesian nonparametric methods, a class of methods that side-steps this issue by allowing the data to determine the complexity of the model. This tutorial is a high-level introduction to Bayesian nonparametric methods and contains several examples of their application.},
author = {Gershman, Samuel J and Blei, David M},
file = {:home/dan/edu/mendeley/Gershman, Blei - 2011 - A Tutorial on Bayesian Nonparametric Models.pdf:pdf},
title = {{A Tutorial on Bayesian Nonparametric Models}},
url = {https://arxiv.org/pdf/1106.2697.pdf},
year = {2011}
}
@article{Mairal,
abstract = {Sparse coding—that is, modelling data vectors as sparse linear combinations of basis elements—is widely used in machine learning, neuroscience, signal processing, and statistics. This paper fo-cuses on learning the basis set, also called dic-tionary, to adapt it to specific data, an approach that has recently proven to be very effective for signal reconstruction and classification in the au-dio and image processing domains. This paper proposes a new online optimization algorithm for dictionary learning, based on stochastic ap-proximations, which scales up gracefully to large datasets with millions of training samples. A proof of convergence is presented, along with experiments with natural images demonstrating that it leads to faster performance and better dic-tionaries than classical batch algorithms for both small and large datasets.},
author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo},
file = {:home/dan/edu/mendeley/Mairal et al. - Unknown - Online Dictionary Learning for Sparse Coding.pdf:pdf},
title = {{Online Dictionary Learning for Sparse Coding}},
url = {http://www.di.ens.fr/sierra/pdfs/icml09.pdf}
}
@article{Lee,
abstract = {Sparse coding provides a class of algorithms for finding succinct representations of stimuli; given only unlabeled input data, it discovers basis functions that cap-ture higher-level features in the data. However, finding sparse codes remains a very difficult computational problem. In this paper, we present efficient sparse coding algorithms that are based on iteratively solving two convex optimization problems: an L 1 -regularized least squares problem and an L 2 -constrained least squares problem. We propose novel algorithms to solve both of these optimiza-tion problems. Our algorithms result in a significant speedup for sparse coding, allowing us to learn larger sparse codes than possible with previously described algorithms. We apply these algorithms to natural images and demonstrate that the inferred sparse codes exhibit end-stopping and non-classical receptive field sur-round suppression and, therefore, may provide a partial explanation for these two phenomena in V1 neurons.},
author = {Lee, Honglak and Battle, Alexis and Raina, Rajat and Ng, Andrew Y},
file = {:home/dan/edu/mendeley/Lee et al. - Unknown - Efficient sparse coding algorithms.pdf:pdf},
title = {{Efficient sparse coding algorithms}},
url = {https://papers.nips.cc/paper/2979-efficient-sparse-coding-algorithms.pdf}
}
@article{Huang2018,
author = {Huang, Yulong and Zhang, Yonggang and Wu, Zhemin and Li, Ning and Chambers, Jonathon},
doi = {10.1109/TAC.2017.2730480},
issn = {0018-9286},
journal = {IEEE Transactions on Automatic Control},
month = {feb},
number = {2},
pages = {594--601},
title = {{A Novel Adaptive Kalman Filter With Inaccurate Process and Measurement Noise Covariance Matrices}},
url = {http://ieeexplore.ieee.org/document/8025799/},
volume = {63},
year = {2018}
}
@incollection{nonparabayes,
author = {Zhou, Mingyuan and Chen, Haojun and Ren, Lu and Sapiro, Guillermo and Carin, Lawrence and Paisley, John W},
booktitle = {Advances in Neural Information Processing Systems 22},
editor = {Bengio, Y and Schuurmans, D and Lafferty, J and Williams, C and Culotta, A},
file = {:home/dan/edu/mendeley/Zhou - 2012 - Non-Parametric Bayesian Dictionary Learning for Sparse Image Representations (1).pdf:pdf},
pages = {2295--2303},
title = {{Non-Parametric Bayesian Dictionary Learning for Sparse Image Representations}},
url = {http://books.nips.cc/papers/files/nips22/NIPS2009{\_}0190.pdf},
year = {2009}
}
@article{Liang,
annote = {https://github.com/dawenl/bp{\_}dict{\_}learn/blob/master/bpdl{\_}sampler.py},
author = {Liang, Dawen},
file = {:home/dan/edu/mendeley/Liang - Unknown - Nonparametric Bayesian Dictionary Learning for Machine Listening.pdf:pdf},
title = {{Nonparametric Bayesian Dictionary Learning for Machine Listening}},
url = {http://www.ee.columbia.edu/{~}dliang/files/E6886{\_}sparse.pdf}
}
@article{Liu2013,
abstract = {We consider the following signal recovery problem: given a measurement matrix {\$}\backslashPhi\backslashin \backslashmathbb{\{}R{\}}{\^{}}{\{}n\backslashtimes p{\}}{\$} and a noisy observation vector {\$}c\backslashin \backslashmathbb{\{}R{\}}{\^{}}{\{}n{\}}{\$} constructed from {\$}c = \backslashPhi\backslashtheta{\^{}}* + \backslashepsilon{\$} where {\$}\backslashepsilon\backslashin \backslashmathbb{\{}R{\}}{\^{}}{\{}n{\}}{\$} is the noise vector whose entries follow i.i.d. centered sub-Gaussian distribution, how to recover the signal {\$}\backslashtheta{\^{}}*{\$} if {\$}D\backslashtheta{\^{}}*{\$} is sparse {\{}$\backslash$rca under a linear transformation{\}} {\$}D\backslashin\backslashmathbb{\{}R{\}}{\^{}}{\{}m\backslashtimes p{\}}{\$}? One natural method using convex optimization is to solve the following problem: {\$}{\$}$\backslash$min{\_}{\{}$\backslash$theta{\}} {\{}1$\backslash$over 2{\}}$\backslash$|$\backslash$Phi$\backslash$theta - c$\backslash$|{\^{}}2 + $\backslash$lambda$\backslash$|D$\backslash$theta$\backslash$|{\_}1.{\$}{\$} This paper provides an upper bound of the estimate error and shows the consistency property of this method by assuming that the design matrix {\$}\backslashPhi{\$} is a Gaussian random matrix. Specifically, we show 1) in the noiseless case, if the condition number of {\$}D{\$} is bounded and the measurement number {\$}n\backslashgeq \backslashOmega(s\backslashlog(p)){\$} where {\$}s{\$} is the sparsity number, then the true solution can be recovered with high probability; and 2) in the noisy case, if the condition number of {\$}D{\$} is bounded and the measurement increases faster than {\$}s\backslashlog(p){\$}, that is, {\$}s\backslashlog(p)=o(n){\$}, the estimate error converges to zero with probability 1 when {\$}p{\$} and {\$}s{\$} go to infinity. Our results are consistent with those for the special case {\$}D=\backslashbold{\{}I{\}}{\_}{\{}p\backslashtimes p{\}}{\$} (equivalently LASSO) and improve the existing analysis. The condition number of {\$}D{\$} plays a critical role in our analysis. We consider the condition numbers in two cases including the fused LASSO and the random graph: the condition number in the fused LASSO case is bounded by a constant, while the condition number in the random graph case is bounded with high probability if {\$}m\backslashover p{\$} (i.e., {\$}{\#}text{\{}edge{\}}\backslashover {\#}text{\{}vertex{\}}{\$}) is larger than a certain constant. Numerical simulations are consistent with our theoretical results.},
archivePrefix = {arXiv},
arxivId = {1305.0047},
author = {Liu, Ji and Yuan, Lei and Ye, Jieping},
eprint = {1305.0047},
file = {:home/dan/edu/mendeley/Liu, Yuan, Ye - 2013 - Dictionary LASSO Guaranteed Sparse Recovery under Linear Transformation.pdf:pdf},
month = {apr},
title = {{Dictionary LASSO: Guaranteed Sparse Recovery under Linear Transformation}},
url = {http://arxiv.org/abs/1305.0047},
year = {2013}
}
@article{Campbell2016,
author = {Campbell, Mark E. and Ahmed, Nisar R.},
doi = {10.1109/MCS.2016.2558444},
issn = {1066-033X},
journal = {IEEE Control Systems},
month = {aug},
number = {4},
pages = {83--109},
title = {{Distributed Data Fusion: Neighbors, Rumors, and the Art of Collective Knowledge}},
url = {http://ieeexplore.ieee.org/document/7515322/},
volume = {36},
year = {2016}
}
@article{Yu2012,
author = {Yu, Myeong-Jong},
doi = {10.1109/TAES.2012.6178100},
issn = {0018-9251},
journal = {IEEE Transactions on Aerospace and Electronic Systems},
number = {2},
pages = {1786--1792},
title = {{INS/GPS Integration System using Adaptive Filter for Estimating Measurement Noise Variance}},
url = {http://ieeexplore.ieee.org/document/6178100/},
volume = {48},
year = {2012}
}
@article{Coetzee2013,
author = {Coetzee, Johann F},
doi = {10.1016/j.cvfa.2012.11.002},
file = {:home/dan/edu/mendeley/korelac.pdf:pdf},
isbn = {2009651200},
issn = {0749-0720},
journal = {Veterinary Clinics of NA: Food Animal Practice},
keywords = {analgesia animal welfare,castration cattle pain assessment},
number = {1},
pages = {75--101},
title = {{A s s e s s m e n t an d M a n a g e m e n t o f Pai n A ss o ci ate d w it h C as tr ati o n in Ca ttle Castration Cattle Pain assessment Analgesia Animal welfare}},
url = {http://dx.doi.org/10.1016/j.cvfa.2012.11.002},
volume = {29},
year = {2013}
}
@article{Gannot2012,
author = {Gannot, S.},
doi = {10.1109/MIM.2012.6204866},
issn = {1094-6969},
journal = {IEEE Instrumentation {\&} Measurement Magazine},
month = {jun},
number = {3},
pages = {10--14},
title = {{Speech processing utilizing the Kalman filter}},
url = {http://ieeexplore.ieee.org/document/6204866/},
volume = {15},
year = {2012}
}
@incollection{Govaers2018,
author = {Govaers, Felix},
booktitle = {Kalman Filters - Theory for Advanced Applications},
doi = {10.5772/intechopen.71941},
file = {:home/dan/edu/mendeley/Govaers - 2018 - Distributed Kalman Filter.pdf:pdf},
month = {feb},
publisher = {InTech},
title = {{Distributed Kalman Filter}},
url = {http://www.intechopen.com/books/kalman-filters-theory-for-advanced-applications/distributed-kalman-filter},
year = {2018}
}
@book{finettitheory,
author = {de Finetti, B},
publisher = {J. Wiley {\&} Sons, Inc., New York},
title = {{Theory of Probability: A critical introductory treatment}},
year = {1975}
}
@article{Zhou2009,
author = {Zhou, Mingyuan and Chen, Haojun and Paisley, John and Ren, Lu and Sapiro, Guillermo and Carin, Lawrence},
file = {:home/dan/edu/mendeley/Model and Inference.pdf:pdf},
journal = {Nips},
number = {2},
pages = {1--4},
title = {{Non-Parametric Bayesian Dictionary Learning for Sparse Image Representations: Model and Inference}},
year = {2009}
}
@article{Hoffmann2007,
abstract = {Kernel principal component analysis (kernel PCA) is a non-linear extension of PCA. This study introduces and investigates the use of kernel PCA for novelty detection. Training data are mapped into an infinite-dimensional feature space. In this space, kernel PCA extracts the principal components of the data distribution. The squared distance to the corresponding principal subspace is the measure for novelty. This new method demonstrated a competitive performance on two-dimensional synthetic distributions and on two real-world data sets: handwritten digits and breast-cancer cytology. {\textcopyright} 2006 Pattern Recognition Society.},
author = {Hoffmann, Heiko},
doi = {10.1016/j.patcog.2006.07.009},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Breast cancer,Handwritten digit,Kernel method,Novelty detection,PCA},
number = {3},
pages = {863--874},
title = {{Kernel PCA for novelty detection}},
volume = {40},
year = {2007}
}
