#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\begin_preamble
\input{preamble}
\end_preamble
\use_default_options true
\master Main.lyx
\begin_modules
todonotes
theorems-ams
theorems-ams-extended
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "default" "default"
\font_sans "lmss" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics pdftex
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\branch childonly2
\selected 1
\filename_suffix 0
\color #faf0e6
\end_branch
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 2
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Chapter
Sparse dictionary learning
\end_layout

\begin_layout Standard
This chapter serves as a preliminary introduction to sparse dictionary learning.
 
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
Should this be a 
\begin_inset Quotes eld
\end_inset

Sparce DL
\begin_inset Quotes erd
\end_inset

 or just 
\begin_inset Quotes eld
\end_inset

Dictionary learning
\begin_inset Quotes erd
\end_inset

? 
\end_layout

\end_inset


\end_layout

\begin_layout Section
Introduction
\begin_inset CommandInset label
LatexCommand label
name "sec:Introduction"

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
Surprisingly little literature on this topic (even less in Czech).
 Something is under 
\begin_inset Quotes eld
\end_inset

Řídká reprezentace signálu
\begin_inset Quotes erd
\end_inset

 and almost always related to images (this is the case for English literature
 as well though).
 
\end_layout

\end_inset


\end_layout

\begin_layout Standard
In many real world mathematical challenges, one wish to choose the best
 data representation which fits best to the problem.
 Sparse dictionary learning is a method, or rather, a family of methods
 and algorithms to find a specific kind of representation.
 As can be derived from the name, this representation is based on some 
\emph on
dictionary
\emph default
, whose elements (called 
\emph on
atoms
\emph default
) are used to compose the original data and the representation is supposed
 to be 
\emph on
sparse
\emph default
, which usually means having as few non-zero elements as possible (or at
 least 
\emph on
close enough
\emph default
 to zero).
 Various definitions depending on the current task are possible of course.
\end_layout

\begin_layout Standard
Sparse representations had been historically mostly used in signal processing
 where one wants to find a such representation with as few components as
 possible.
 Dictionaries were in these cases selected manually before observing the
 data (e.g.
 Fourier transforms).
 Contrary to that, dictionary 
\emph on
learning
\emph default
 generates this dictionary based on the data, potentially leading to even
 more efficient representation tied closely to the data structure.
 
\end_layout

\begin_layout Standard
Sparse representations are being widely used in multimedia processing, such
 as denoising and classifying images or compression.
 Having an ability to store one external dictionary and then possibly represent
 every data point just by few coefficients can also dramatically reduce
 memory or storage usage in cases when this is a concern.
 Sparse representations also favor simple models, a useful property in e.g.
 classification tasks to fight issues with over-training
\begin_inset CommandInset citation
LatexCommand cite
key "Zhou2009"
literal "true"

\end_inset

.
 The sparse coefficients may also posses a physical meaning in case on is
 interested in model interpretation
\begin_inset CommandInset citation
LatexCommand cite
key "Olshausen1998"
literal "true"

\end_inset

.
\end_layout

\begin_layout Section
The problem
\end_layout

\begin_layout Standard
Sparse dictionary learning can be also interpreted as the following optimization
 problem:
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
This chapter mostly rewriten/copied from wikipedia...
 :-( But there is maybe a better variant in 
\begin_inset CommandInset citation
LatexCommand cite
key "MairalONDL"
literal "true"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Problem
\begin_inset CommandInset label
LatexCommand label
name "prob:sparce-repre"

\end_inset


\begin_inset Formula $X=[x_{1},...,x_{K}],x_{i}\in\mathbb{R}^{d}$
\end_inset

, we want to find 
\begin_inset Formula $\mathbf{D}\in\mathbb{R}^{d\times n}:D=[d_{1},...,d_{n}]$
\end_inset

 and a representation 
\begin_inset Formula $R=[r_{1},...,r_{K}],r_{i}\in\mathbb{R}^{n}$
\end_inset

 such that both 
\begin_inset Formula $\|X-\mathbf{D}R\|_{F}^{2}$
\end_inset

 is minimized and the representations 
\begin_inset Formula $r_{i}$
\end_inset

 are sparse enough:
\end_layout

\begin_layout Problem
\begin_inset Formula 
\[
\ensuremath{\underset{\mathbf{D}\in\mathcal{C},r_{i}\in\mathbb{R}^{n}}{\text{argmin}}\sum_{i=1}^{K}\|x_{i}-\mathbf{D}r_{i}\|_{2}^{2}+\lambda\|r_{i}\|_{0}}
\]

\end_inset


\end_layout

\begin_layout Problem
where 
\begin_inset Formula $\mathcal{C}\equiv\{\mathbb{D}\in\mathbb{R}^{d\times n}:\|d_{i}\|_{2}\leq1\forall i=1,...,n\}$
\end_inset

.
 
\end_layout

\begin_layout Problem
\begin_inset Formula $\mathcal{C}$
\end_inset

 is required to constrain 
\begin_inset Formula $\mathbf{D}$
\end_inset

 so that its atoms would not reach arbitrarily high values allowing for
 arbitrarily low (but non-zero) values of 
\begin_inset Formula $r_{i}$
\end_inset

.
 Problem 
\begin_inset CommandInset ref
LatexCommand ref
reference "prob:sparce-repre"

\end_inset

 stated as above is unfortunately not convex (because of 
\begin_inset Formula $p=0$
\end_inset

 
\begin_inset Quotes eld
\end_inset

norm
\begin_inset Quotes erd
\end_inset

) and solving this is NP-hard
\begin_inset CommandInset citation
LatexCommand cite
key "MairalONDL"
literal "true"

\end_inset

.
 
\end_layout

\begin_layout Problem
It is known that using 
\begin_inset Formula $L^{1}$
\end_inset

-norm can also lead to sparse solutions converting this problem to convex
 optimization
\begin_inset CommandInset citation
LatexCommand cite
key "sparsesolsl1"
literal "true"

\end_inset

.
 But that can be done only with respect to either 
\begin_inset Formula $\mathbf{D}$
\end_inset

 or 
\begin_inset Formula $\mathbf{R}$
\end_inset

 while the other one is fixed.
 
\end_layout

\begin_layout Section
Sparse dictionaries
\end_layout

\begin_layout Standard
As has been mentioned in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Introduction"

\end_inset

, sparse dictionary 
\begin_inset Formula $\mathbf{D}$
\end_inset

 described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "prob:sparce-repre"

\end_inset

 can have various forms depending on the final goal because of which is
 the dictionary being created.
 Probably the top level difference is if one is searching for so called
 
\emph on
undercomplete 
\emph default
– where 
\begin_inset Formula $n<d$
\end_inset

, or 
\emph on
overcomplete
\emph default
 
\begin_inset Formula $n>d$
\end_inset

 dictionary
\begin_inset Foot
status open

\begin_layout Plain Layout
Case where 
\begin_inset Formula $d=n$
\end_inset

 is not 
\begin_inset Quotes eld
\end_inset

special
\begin_inset Quotes erd
\end_inset

 in any notable way for us.
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Subsection*
Undercomplete dictionaries
\end_layout

\begin_layout Standard
Using fewer components than is the dimension of the original input data
 is basically a dimensionality reduction problem and there have been numerous
 techniques developed over the years.
 Probably the most famous one is Principal component analysis (PCA), which
 poses additional requirement on dictionary atoms and that is their mutual
 orthogonality (which is possible, since 
\begin_inset Formula $d\le n$
\end_inset

).
 Data is linearly mapped to lower dimension in a way that the variance of
 the data is maximized.
 Another examples are Kernel PCA (which is an non-linear extension of PCA),
 linear discriminant analysis and others.
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
should I cite here and refer the reader somewhere?
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The disadvantages are obvious – by representing data in the subspace, one
 can (and usually does) loose too much of information.
 In PCA, the choice of number of components is quite hard without being
 able to measure how much information is going to be lost while using less
 and less of them.
 Some heuristics exists, but they are more rule-of-thumb rather then rigorous
 approach.
 
\begin_inset Flex TODO Note (inline)
status collapsed

\begin_layout Plain Layout
what about an example of PCA? again nice chart
\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
Overcomplete dictionaries
\end_layout

\begin_layout Standard
Contrary to undercomplete dictionaries, overcomplete ones cannot be orthogonal
 by definition, since 
\begin_inset Formula $d>n$
\end_inset

.
 Because of this fact, overcomplete dictionary is more flexible and hence
 is able to better adapt to the data allowing to capture more subtle details.
 The trade off here is the size of the dictionary.
 One can imagine an extreme case in which he would create a dictionary from
 the all input data.
 This representation would surely allow extremely sparse representation
 of any input vector, using only a single non-zero coefficient:
\begin_inset Formula $1$
\end_inset

 on a corresponding position, but we can imagine that this is representation
 does not bring us any advantages in real usage
\begin_inset Foot
status collapsed

\begin_layout Plain Layout
We just added one more useless step – a lookup in a dictionary – before
 we want to do anything with data
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Standard
Overcomplete dictionaries are usually the ones assumed when talking about
 sparse dictionary and it is a case of this work as well.
 Methods which are being used for this purpose is for example Fourier transform
\begin_inset Flex TODO Note (Margin)
status open

\begin_layout Plain Layout
Right? Theoretically infinite.
\end_layout

\end_inset

, respective its variant to obtain sparse solutions.
 A distinction must be made though, because Fourier transform is an example
 of method where one has a predefined dictionary, which is not the case
 of methods this work is mostly concerned with.
\end_layout

\begin_layout Itemize
simple example? 
\begin_inset Flex TODO Note (inline)
status open

\begin_layout Plain Layout
examples for both cases as well?
\end_layout

\end_inset


\end_layout

\begin_layout Section
Methods of dictionary learning
\end_layout

\begin_layout Standard
\begin_inset Flex TODO Note (inline)
status collapsed

\begin_layout Plain Layout
shrnutí ano, nic dlouhého (jako v článku)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Many different methods are being used in practice based on the details of
 the problem they are applied to.
 For instance, some methods are more useful when used in image reconstruction,
 while others are a better fit for classification tasks or factor analysis.
 A short summary with advantages and disadvantages, if available, follows.
\end_layout

\begin_layout Description
K-SVD Is basically a generalization of K-means clustering
\begin_inset CommandInset citation
LatexCommand cite
key "J.MacQueen1967"
literal "false"

\end_inset

, the dictionary is being found by the nearest neighbors algorithm.
\end_layout

\begin_layout Enumerate
K-SVD
\end_layout

\begin_layout Enumerate
mod
\end_layout

\begin_layout Enumerate
stichastic gradient descend
\end_layout

\begin_layout Enumerate
lagrange dual method...
\end_layout

\begin_layout Standard
\begin_inset Branch childonly2
inverted 0
status collapsed

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Nemazat - je to potřeba abych viděl citace v \SpecialChar LyX

\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "library"
options "bibtotoc,plain"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
printbibliography
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_body
\end_document
