#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\begin_preamble
\input{preamble}
\end_preamble
\use_default_options true
\master Main.lyx
\begin_modules
todonotes
theorems-ams
theorems-ams-extended
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding utf8
\fontencoding global
\font_roman "default" "default"
\font_sans "lmss" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics pdftex
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 2
\use_package amssymb 2
\use_package cancel 1
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\use_minted 0
\branch childonly2
\selected 1
\filename_suffix 0
\color #faf0e6
\end_branch
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 2
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Chapter
Sparse dictionary learning
\end_layout

\begin_layout Standard
This chapter serves as a preliminary introduction to sparse dictionary learning.
 
\end_layout

\begin_layout Section
Introduction
\begin_inset CommandInset label
LatexCommand label
name "sec:Introduction"

\end_inset


\end_layout

\begin_layout Standard
In many real world mathematical challenges, one wish to choose the best
 data representation which fits best to the problem.
 Sparse dictionary learning is a method, or rather, a family of methods
 and algorithms to find a specific kind of representation.
 As can be derived from the name, this representation is based on some 
\emph on
dictionary
\emph default
, whose elements (called 
\emph on
atoms
\emph default
) are used to compose the original data and the representation is supposed
 to be 
\emph on
sparse
\emph default
, which usually means having as few non-zero elements as possible (or at
 least 
\emph on
close enough
\emph default
 to zero).
 Various definitions depending on the current task are possible of course.
\end_layout

\begin_layout Standard
Sparse representations had been historically mostly used in signal processing
 where one wants to find a such representation with as few components as
 possible.
 Dictionaries were in these cases selected manually before observing the
 data (e.g.
 Fourier transforms).
 Contrary to that, dictionary 
\emph on
learning
\emph default
 generates this dictionary based on the data, potentially leading to even
 more efficient representation tied closely to the data structure.
 
\end_layout

\begin_layout Standard
Sparse representations are being widely used in multimedia processing, such
 as denoising and classifying images or compression.
 Having an ability to store one external dictionary and then possibly represent
 every data point just by few coefficients can also dramatically reduce
 memory or storage usage in cases when this is a concern.
 Sparse representations also favor simple models, a useful property in e.g.
 classification tasks to fight issues with over-training
\begin_inset CommandInset citation
LatexCommand cite
key "Zhou2009"
literal "true"

\end_inset

.
 The sparse coefficients may also posses a physical meaning in case on is
 interested in model interpretation
\begin_inset CommandInset citation
LatexCommand cite
key "Olshausen1998"
literal "true"

\end_inset

.
\end_layout

\begin_layout Section
The problem
\end_layout

\begin_layout Standard
Sparse dictionary learning can be also interpreted as the following optimization
 problem:
\begin_inset Flex TODO Note (inline)
status collapsed

\begin_layout Plain Layout
This chapter mostly rewriten/copied from wikipedia...
 :-( But there is maybe a better variant in 
\begin_inset CommandInset citation
LatexCommand cite
key "MairalONDL"
literal "true"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Problem
\begin_inset CommandInset label
LatexCommand label
name "prob:sparce-repre"

\end_inset


\begin_inset Formula $X=[x_{1},...,x_{K}],x_{i}\in\mathbb{R}^{d}$
\end_inset

, we want to find 
\begin_inset Formula $\mathbf{D}\in\mathbb{R}^{d\times n}:D=[d_{1},...,d_{n}]$
\end_inset

 and a representation 
\begin_inset Formula $R=[r_{1},...,r_{K}],r_{i}\in\mathbb{R}^{n}$
\end_inset

 such that both 
\begin_inset Formula $\|X-\mathbf{D}R\|_{F}^{2}$
\end_inset

 is minimized and the representations 
\begin_inset Formula $r_{i}$
\end_inset

 are sparse enough:
\end_layout

\begin_layout Problem
\begin_inset Formula 
\[
\ensuremath{\underset{\mathbf{D}\in\mathcal{C},r_{i}\in\mathbb{R}^{n}}{\text{argmin}}\sum_{i=1}^{K}\|x_{i}-\mathbf{D}r_{i}\|_{2}^{2}+\lambda\|r_{i}\|_{0}}
\]

\end_inset


\end_layout

\begin_layout Problem
where 
\begin_inset Formula $\mathcal{C}\equiv\{\mathbb{D}\in\mathbb{R}^{d\times n}:\|d_{i}\|_{2}\leq1\forall i=1,...,n\}$
\end_inset

.
 
\end_layout

\begin_layout Problem
\begin_inset Formula $\mathcal{C}$
\end_inset

 is required to constrain 
\begin_inset Formula $\mathbf{D}$
\end_inset

 so that its atoms would not reach arbitrarily high values allowing for
 arbitrarily low (but non-zero) values of 
\begin_inset Formula $r_{i}$
\end_inset

.
 Problem 
\begin_inset CommandInset ref
LatexCommand ref
reference "prob:sparce-repre"

\end_inset

 stated as above is unfortunately not convex (because of 
\begin_inset Formula $p=0$
\end_inset

 
\begin_inset Quotes eld
\end_inset

norm
\begin_inset Quotes erd
\end_inset

) and solving this is NP-hard
\begin_inset CommandInset citation
LatexCommand cite
key "MairalONDL"
literal "true"

\end_inset

.
 
\end_layout

\begin_layout Problem
It is known that using 
\begin_inset Formula $L^{1}$
\end_inset

-norm can also lead to sparse solutions converting this problem to convex
 optimization
\begin_inset CommandInset citation
LatexCommand cite
key "sparsesolsl1"
literal "true"

\end_inset

.
 But that can be done only with respect to either 
\begin_inset Formula $\mathbf{D}$
\end_inset

 or 
\begin_inset Formula $\mathbf{R}$
\end_inset

 while the other one is fixed.
 
\end_layout

\begin_layout Section
Sparse dictionaries
\end_layout

\begin_layout Standard
As has been mentioned in section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Introduction"

\end_inset

, sparse dictionary 
\begin_inset Formula $\mathbf{D}$
\end_inset

 described in 
\begin_inset CommandInset ref
LatexCommand ref
reference "prob:sparce-repre"

\end_inset

 can have various forms depending on the final goal because of which is
 the dictionary being created.
 Probably the top level difference is if one is searching for so called
 
\emph on
undercomplete 
\emph default
– where 
\begin_inset Formula $n<d$
\end_inset

, or 
\emph on
overcomplete
\emph default
 
\begin_inset Formula $n>d$
\end_inset

 dictionary.
 Case where 
\begin_inset Formula $d=n$
\end_inset

 is not special in any notable way for us.
 
\end_layout

\begin_layout Subsection*
Undercomplete dictionaries
\end_layout

\begin_layout Standard
Using fewer components than is the dimension of the original input data
 is basically a dimensionality reduction problem and there have been numerous
 techniques developed over the years.
 Probably the most famous one is Principal component analysis (PCA), which
 poses additional requirement on dictionary atoms and that is their mutual
 orthogonality (which is possible, since 
\begin_inset Formula $d\le n$
\end_inset

) 
\begin_inset CommandInset citation
LatexCommand cite
key "Jolliffe2002"
literal "false"

\end_inset

.
 Data is linearly mapped to lower dimension in a way that the variance of
 the data is maximized.
 An example can be seen on figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:pca-example"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Another examples are Kernel PCA (which is an non-linear extension of PCA)
 
\begin_inset CommandInset citation
LatexCommand cite
key "Hoffmann2007"
literal "false"

\end_inset

, linear discriminant analysis and others.
\end_layout

\begin_layout Standard
The disadvantages are obvious – by representing data in the subspace, one
 can (and usually does) loose too much of information.
 In PCA, the choice of number of components is quite hard without being
 able to measure how much information is going to be lost while using less
 and less of them.
 Some heuristics exists, but they are more rule-of-thumb rather then rigorous
 approach.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename /home/dan/edu/uni/diplomka/nbs/pca-example.pdf
	width 60page%
	height 60page%
	keepAspectRatio
	groupId 60ts

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:pca-example"

\end_inset

An example of applying PCA on a Boston house prices dataset and visualizing
 an amount of explained variance ratio per component.
 It can be seen, that in 
\series bold
a matter of the explained variance only
\series default
, one can get majority of the information just from the first 4 components.
 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
Overcomplete dictionaries
\end_layout

\begin_layout Standard
Contrary to undercomplete dictionaries, overcomplete ones cannot be orthogonal
 by definition, since 
\begin_inset Formula $d>n$
\end_inset

.
 Because of this fact, overcomplete dictionary is more flexible and hence
 is able to better adapt to the data allowing to capture more subtle details.
 The trade off here is the size of the dictionary.
 One can imagine an extreme case in which he would create a dictionary from
 the all input data.
 This representation would surely allow extremely sparse representation
 of any input vector, using only a single non-zero coefficient:
\begin_inset Formula $1$
\end_inset

 on a corresponding position, but we can imagine that this is representation
 does not bring us any advantages in real usage.
 We just added one more useless step, a lookup in a dictionary, before being
 able to do anything with a data.
 
\end_layout

\begin_layout Standard
Overcomplete dictionaries are usually the ones assumed when talking about
 sparse dictionaries and it is the case of this work as well.
 A notable example of a widely used method is a Fourier transform
\begin_inset Flex TODO Note (Margin)
status collapsed

\begin_layout Plain Layout
Right? Theoretically infinite.
\end_layout

\end_inset

, respective its variant to obtain sparse solutions.
 A distinction must be made though, because Fourier transform is an example
 of an algorithm where one has a predefined dictionary.
 That is not the case of dictionary learning, where the dictionary is being
 constructed in an unsupervised fashion.
 A brief description of these methods follows in chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Methods-of-sparse"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 
\end_layout

\begin_layout Section
Methods of sparse dictionary learning
\begin_inset CommandInset label
LatexCommand label
name "sec:Methods-of-sparse"

\end_inset


\end_layout

\begin_layout Standard
Many different methods are being used in practice based on the details of
 the problem they are applied to.
 For instance, some methods are more useful when used in image reconstruction,
 while others are a better fit for classification tasks or factor analysis.
 A short summary with advantages and disadvantages, if available, follows.
 We refer the reader to a appropriate literature for a more detailed explanation.
 
\end_layout

\begin_layout Description
Method
\begin_inset space ~
\end_inset

of
\begin_inset space ~
\end_inset

optimal
\begin_inset space ~
\end_inset

directions
\begin_inset space ~
\end_inset

(MOD) One of the first methods developed for dictionary learning 
\begin_inset CommandInset citation
LatexCommand cite
key "Engan1999"
literal "false"

\end_inset

.
 It tries to solve the problem 
\begin_inset CommandInset ref
LatexCommand ref
reference "prob:sparce-repre"
plural "false"
caps "false"
noprefix "false"

\end_inset

 with a specific number of non-zero elements of the representation vector.
 The computation requires computing Moore-Penrose pseudoinverse matrix,
 which makes the method intractable for higher dimensions of the data.
 Nevertheless, it does prove useful when considering low-dimensional data.
\end_layout

\begin_layout Description
K-SVD Is basically a generalization of K-means clustering 
\begin_inset CommandInset citation
LatexCommand cite
key "MacQueen1967"
literal "false"

\end_inset

, the dictionary is created by the nearest neighbors algorithm.
 First, the current dictionary is fixed, then the best sparse coding is
 found and then the atoms are updated one by one.
 Since choosing the dictionary is a non-convex problem and updates are iterative
, it is not guaranteed that a global minimum is found.
 Additionally, same as MOD, it does not scale very well in respect to the
 dimensionality of a data.
 This algorithm is considered to be a standard in dictionary learning.
 
\begin_inset CommandInset citation
LatexCommand cite
key "Aharon2006"
literal "false"

\end_inset


\end_layout

\begin_layout Description
Stochastic
\begin_inset space ~
\end_inset

Gradient
\begin_inset space ~
\end_inset

Descent This widely used algorithm in many areas of optimization theory
 can be as well used for a dictionary learning problems 
\begin_inset CommandInset citation
LatexCommand cite
key "Aharon2003"
literal "false"

\end_inset

.
 The dictionary is updated based on the first order stochastic gradient
 descent and projected on a constraint set.
 Similarly to K-SVD, there is no guarantee of reaching global minimum.
 
\end_layout

\begin_layout Description
Lagrangian
\begin_inset space ~
\end_inset

Dual
\begin_inset space ~
\end_inset

Method The core of this method lies in solving a dual Lagrangian problem.
 Instead of solving the primal problem, one constructs a dual Lagrangian
 and minimize that by different minimization methods to get values of the
 dual.
 This is much easier, since the amount of dual variables is much smaller
 than in the primal problem 
\begin_inset CommandInset citation
LatexCommand cite
key "Lee"
literal "false"

\end_inset

.
\end_layout

\begin_layout Description
LARS-Lasso Similarly to the previous methods, LARS-Lasso based algorithms
 are not specific for dictionary learning problems, but can be used to solve
 the problem 
\begin_inset CommandInset ref
LatexCommand ref
reference "prob:sparce-repre"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 There are multiple proposed algorithms based on these methods such as 
\begin_inset CommandInset citation
LatexCommand cite
key "Liu2013"
literal "false"

\end_inset

 or an efficient online dictionary learning algorithm described in 
\begin_inset CommandInset citation
LatexCommand cite
key "Mairal"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Branch childonly2
inverted 0
status collapsed

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
Nemazat - je to potřeba abych viděl citace v \SpecialChar LyX

\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "library"
options "bibtotoc,plain"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
printbibliography
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_body
\end_document
